% Copyright 2015-2017 Dan Foreman-Mackey and the co-authors listed below.

\documentclass[manuscript, letterpaper]{aastex6}

\pdfoutput=1

\include{vc}
\usepackage{microtype}

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% ----------------------------------- %
% start of AASTeX mods by DWH and DFM %
% ----------------------------------- %

% Matrix fix:
% http://tex.stackexchange.com/questions/317824/letter-c-appearing-inside-pmatrix-environment-with-aastex
\makeatletter
\def\env@matrix{\hskip -\arraycolsep % taken from amsmath.sty lines 895ff
  \let\@ifnextchar\new@ifnextchar
  \array{*{\c@MaxMatrixCols}c}}
\makeatother

% Column spacing in matrix
% http://tex.stackexchange.com/questions/275725/adjusting-separation-between-matrix-entries
\setlength\arraycolsep{25pt}

\setlength{\voffset}{0in}
\setlength{\hoffset}{0in}
\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\headheight}{0ex}
\setlength{\headsep}{\baselinestretch\baselineskip} % this is 2 lines in ``manuscript''
\setlength{\footnotesep}{0in}
\setlength{\topmargin}{-\headsep}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}

\linespread{0.54} % close to 10/13 spacing in ``manuscript''
\setlength{\parindent}{0.54\baselineskip}
\hypersetup{colorlinks = false}
\makeatletter % you know you are living your life wrong when you need to do this
\long\def\frontmatter@title@above{
\vspace*{-\headsep}\vspace*{\headheight}
\noindent\footnotesize
{\noindent\footnotesize\textsc{\@journalinfo}}\par
{\noindent\scriptsize Preprint typeset using \LaTeX\ style AASTeX6 with modifications
}\par\vspace*{-\baselineskip}\vspace*{0.625in}
}%
\makeatother

% Section spacing:
\makeatletter
\let\origsection\section
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}
\newcommand\nostarsection[1]{\sectionprelude\origsection{#1}}
\newcommand\starsection[1]{\sectionprelude\origsection*{#1}}
\newcommand\sectionprelude{\vspace{1em}}
\let\origsubsection\subsection
\renewcommand\subsection{\@ifstar{\starsubsection}{\nostarsubsection}}
\newcommand\nostarsubsection[1]{\subsectionprelude\origsubsection{#1}}
\newcommand\starsubsection[1]{\subsectionprelude\origsubsection*{#1}}
\newcommand\subsectionprelude{\vspace{1em}}
\makeatother

\widowpenalty=10000
\clubpenalty=10000

\sloppy\sloppypar

% ------------------ %
% end of AASTeX mods %
% ------------------ %

% Projects:
\newcommand{\project}[1]{\textsf{#1}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\tess}{\project{TESS}}
\newcommand{\celerite}{\project{celerite}}
\newcommand{\emcee}{\project{emcee}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\sectionname}{Section}
\newcommand{\sectref}[1]{\ref{sect:#1}}
\newcommand{\Sect}[1]{\sectionname~\sectref{#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\sectref{#1}}
\newcommand{\App}[1]{Appendix~\sectref{#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\agoltodo}[1]{\todo{Agol}{blue}{#1}}


% \shorttitle{}
% \shortauthors{}
% \submitted{Submitted to \textit{The Astrophysical Journal}}

\begin{document}

\title{%
Fast and scalable Gaussian process modeling with applications
to astronomical time series
\vspace{-3\baselineskip}  % OMG AASTEX6 IS SO BROKEN
}

\newcounter{affilcounter}
% \altaffiltext{1}{}

\setcounter{affilcounter}{1}

\edef \sagan {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\sagan}{Sagan Fellow}

\edef \uw {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\uw}{Astronomy Department, University of Washington,
                   Seattle, WA}

\edef \simons {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\simons}{Simons Fellow}

\edef \columbia {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\columbia}{Department of Astronomy, Columbia University,
                         New York, NY}


\author{%
    Daniel~Foreman-Mackey\altaffilmark{\sagan,\uw},
    Eric~Agol\altaffilmark{\uw}, and
    Ruth~Angus\altaffilmark{\simons,\columbia}
}



\begin{abstract}

We present a scalable method for Gaussian Process regression in one dimension
with a specific emphasis on large astronomical time-series data sets.
This method can be applied to any Gaussian Process model where the spectral
density can be expressed as any general mixture of damped sinusoid functions.

\end{abstract}

\keywords{%
% methods: data analysis
% ---
% methods: statistical
% ---
% catalogs
% ---
% planetary systems
% ---
% stars: statistics
}

\section{Introduction}

Gaussian Processes \citep[GPs;][]{Rasmussen:2006} are popular stochastic
models for time-series analysis.
For GP modeling, a functional form is chosen to describe the autocovariance
of the data and the parameters of this function are fit for or marginalized.
In the astrophysical literature, GPs have been used to model stochastic
variability in light curves of stars (CITE), active galactic nuclei (CITE),
and X-ray binaries (CITE).
They have also been used as models for the cosmic microwave background (CITE),
correlated instrumental noise (CITE), spectroscopic calibration (CITE) and
residuals caused by model inconsistencies (CITE + better words).
While these models are widely applicable, their use has been limited, in
practice, by the computational cost and scaling.
In general, the cost of computing a GP likelihood scales as the third power of
the number of data points $\mathcal{O}(N^3)$ and in the current era of large
time-domain surveys~--~with $\sim10^{4-9}$ targets with $\sim10^{3-5}$
observations each~---~this cost is prohibitive.

In this paper, we present a class of GP models that enable likelihood
calculations that scale linearly with the number of data points
$\mathcal{O}(N)$ for one dimensional data sets.
This method is a generalization of a method developed by
\citet{Ambikasaran:2015} that was, in turn, built on intuition from a twenty
year old paper \citep{Rybicki:1995}.
For this method to be applicable, the data must be one-dimensional and the
covariance function must have a specific form.
However, there is no further constraint on the data or the model.
In particular, the measurements don't need to be evenly spaced and the
uncertainties can be heteroscedastic.
This method is especially appealing compared to other similar methods~--~we
will return to these below~--~because it is exact, flexible, robust, simple,
and fast.

In the following pages, we will motivate the general problem of GP regression,
describe the previously published scalable method \citep{Rybicki:1995,
Ambikasaran:2015} and our generalization, and demonstrate the model's
application on various real and simulated data sets.
Alongside this paper, we have released efficient and well-tested
implementations of this method written in \project{C++}, \project{Python}, and
\project{Julia}.
These implementations are available online at \project{GitHub}
\url{https://github.com/dfm/celerite} and \project{Zenodo} \dfmtodo{add zenodo
archive}.

\section{Gaussian processes}\sectlabel{gps}

Gaussian Processes \citep[GPs;][]{Rasmussen:2006} are a class of stochastic
models consisting of a mean function $\mu_\bvec{\theta}(\bvec{x})$ and a
covariance or ``kernel'' function $k_\bvec{\alpha}(\bvec{x}_i,\,\bvec{x}_j)$
parameterized by the parameters $\bvec{\theta}$ and $\bvec{\alpha}$
respectively.
Under this model, the log-likelihood of observing a dataset
\begin{eqnarray}
\bvec{y} = \left(\begin{array}{ccccc}
    y_1\quad && \cdots\quad && y_N
\end{array}\right)^\T
\end{eqnarray}
at coordinates
\begin{eqnarray}
X = \left(\begin{array}{ccccc}
    \bvec{x}_1\quad && \cdots\quad && \bvec{x}_N
\end{array}\right)^\T
\end{eqnarray}
is
\begin{eqnarray}\eqlabel{gp-likelihood}
\ln{p(\bvec{y}\,|\,{X,\,\bvec{\theta}},\,\bvec{\alpha})} =
    -\frac{1}{2} {\bvec{r}_\bvec{\theta}}^\T\,{K_\bvec{\alpha}}^{-1}\,
        \bvec{r}_\bvec{\theta}
    -\frac{1}{2}\ln\det K_\bvec{\alpha}
    - \frac{N}{2} \ln{(2\pi)}
\end{eqnarray}
where
\begin{eqnarray}
    \bvec{r}_\bvec{\theta} = \left(\begin{array}{ccccc}
    y_1 - \mu_\bvec{\theta}(\bvec{x}_1)\quad && \cdots\quad &&
    y_N - \mu_\bvec{\theta}(\bvec{x}_N)
\end{array}\right)^\T
\end{eqnarray}
is the vector of residuals and the elements of the covariance matrix $K$ are
given by $[K_\bvec{\alpha}]_{nm} = k_\bvec{\alpha}(\bvec{x}_n,\,\bvec{x}_m)$.
The maximum likelihood values for the parameters $\bvec{\theta}$ and
$\bvec{\alpha}$ for a given dataset $(\bvec{y},\,X)$ can be found by
maximizing \eq{gp-likelihood} with respect to $\bvec{\theta}$ and
$\bvec{\alpha}$ using a non-linear optimization routine \dfmtodo{examples and
CITE}.
Similarly, probabilistic constraints on $\bvec{\theta}$ and $\bvec{\alpha}$
can be obtained by multiplying the likelihood by a prior
$p(\bvec{\theta},\,\bvec{\alpha})$ and using a Markov Chain Monte Carlo (MCMC;
\dfmtodo{CITE}) algorithm to sample from the posterior probability density.

The application of GP models is generally limited to small datasets because
the computational cost of computing the inverse and determinant of the matrix
$K_\bvec{\alpha}$ scales as the cube of the number of data points $N$,
$\mathcal{O}(N^3)$.
This means that for large datasets, every evaluation of the likelihood will
quickly become computationally intractable.
In this case, standard non-linear optimization or MCMC will no longer be
practical inference methods.

In the following Section, we present a method of substantially improving this
scaling in many circumstances.
We call our method and its implementations \celerite.\footnote{The name
\celerite\ comes from the French word \foreign{c\'elerit\'e} meaning the speed
of light in a vacuum.} The \celerite\ method requires using a specific model
for the covariance $k_\bvec{\alpha}(\bvec{x}_n,\,\bvec{x}_m)$ and it has
several limitations but, in subsequent sections, we demonstrate that it can be
used to increase the computational efficiency of many astronomical data
analysis problems.
The main limitation of this method is that it can only be applied to
one-dimensional datasets.
When we say ``one-dimensional'' here, it means that the \emph{input
coordinates} $\bvec{x}_n$ are scalar, $\bvec{x}_n \equiv t_n$.\footnote{We are
using $t$ as the input coordinate because one-dimensional GPs are often
applied to time series data but this isn't a real restriction and the \celerite\
method can be applied to \emph{any} one-dimensional dataset.}
Furthermore, the covariance function for the \celerite\ method is ``stationary''.
This means that the function $k_\bvec{\alpha}(t_n,\,t_m)$ is only a function
of $\tau_{nm} \equiv |t_n - t_m|$.


\section{The celerite model}

To scale GP models to larger datasets, \citet{Rybicki:1995} presented a method
of computing the first term in \eq{gp-likelihood} in $\mathcal{O}(N)$
operations when the covariance function is given by
\begin{eqnarray}\eqlabel{kernel-simple}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} + a\,\exp(-c\,\tau_{nm})
\end{eqnarray}
where $\{{\sigma_n}^2\}_{n=1}^N$ are the measurement uncertainties,
$\delta_{nm}$ is the Kronecker delta, and $\bvec{\alpha} = (a,\,c)$.
The intuition behind this method is that, for this choice of $k_\bvec{\alpha}$,
the inverse of $K_\bvec{\alpha}$ is tridiagonal and can it can be computed
with a small number of operations for each data point.
Subsequently, \citet{Ambikasaran:2015} generalized this method to arbitrary
mixtures of exponentials
\begin{eqnarray}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J a_j\,\exp(-c_j\,\tau_{nm})\quad.
\end{eqnarray}
In this case, the inverse will be dense but \eq{gp-likelihood} can still be
evaluated in $\mathcal{O}(N)$ operations where $J$ is the number of components
in the mixture and $N$ is still the number of data points.

It turns out that this kernel function can be made even more general by
introducing complex parameters $a_j \to a_j\pm i\,b_j$ and
$c_j \to c_j\pm i\,d_j$.
In this case, the covariance function becomes
\begin{eqnarray}\eqlabel{celerite-kernel-complex}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J &&\left[
    \frac{1}{2}(a_j + i\,b_j)\,\exp\left(-(c_j+i\,d_j)\,\tau_{nm}\right)
        \right. \nonumber\\
    &&+\left.
    \frac{1}{2}(a_j - i\,b_j)\,\exp\left(-(c_j-i\,d_j)\,\tau_{nm}\right)
\right]
\end{eqnarray}
and, for this function, \eq{gp-likelihood} can still be solved with
$\mathcal{O}(N)$ operations.
The details of this method and a few implementation considerations are
discussed in the following \sectionname\ but we will first discuss some
properties of this covariance function.

By rewriting the exponentials in \eq{celerite-kernel-complex} as sums of sine
and cosine functions, we can see the autocorrelation structure is defined by a
mixture of quasiperiodic oscillators
\begin{eqnarray}\eqlabel{celerite-kernel}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J &&\left[
    a_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\cos\left(d_j\,\tau_{nm}\right)
        \right.\nonumber\\
    &&+ \left.
    b_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\sin\left(d_j\,\tau_{nm}\right)
\right] \quad.
\end{eqnarray}
For clarity, we will refer to the argument within the sum as a ``\celerite\
term'' for the remainder of this paper.
The Fourier transform\footnote{Here and throughout we have defined the Fourier
transform of the function $f(t)$ as $F(\omega)=(2\,\pi)^{-1/2}\,
\int_{-\infty}^\infty f(t)\,e^{i\,\omega\,t}\dd t$.} of this covariance
function is the power spectral density (PSD) of the process and it is given by
\begin{eqnarray}\eqlabel{celerite-psd}
S(\omega) = \sum_{j=1}^J \sqrt{\frac{2}{\pi}}
\frac{(a_j\,c_j+b_j\,d_j)\,({c_j}^2+{d_j}^2)+(a_j\,c_j-b_j\,d_j)\,\omega^2}
{\omega^4+2\,({c_j}^2-{d_j}^2)\,\omega^2+({c_j}^2+{d_j}^2)^2}\quad.
\end{eqnarray}
The physical interpretation of this model isn't immediately obvious and we
will return to a more general discussion of the physical intuition in a moment
but we can start with a discussion of some useful special cases of this model.

If we set the imaginary amplitude $b_j$ for some component $j$ to zero, that
term of \eq{celerite-kernel} becomes
\begin{eqnarray}
k_j(\tau_{nm}) =
    a_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\cos\left(d_j\,\tau_{nm}\right)
\end{eqnarray}
and the PSD for the this component is
\begin{eqnarray}\eqlabel{lorentz-psd}
S_j(\omega) = \frac{1}{\sqrt{2\,\pi}}\,\frac{a_j}{c_j}\,\left[
    \frac{1}{1+\left(\frac{\omega-d_j}{c_j}\right)^2} +
    \frac{1}{1+\left(\frac{\omega+d_j}{c_j}\right)^2}
\right] \quad.
\end{eqnarray}
This PSD is the sum of two Lorentzian or Cauchy distributions with width
$c_j$ centered on $\omega = \pm d_j$.
This model can be interpreted intuitively as a quasiperiodic oscillator with
amplitude $A_j = a_j$, quality factor $Q_j = d_j\,(2\,c_j)^{-1}$, and period
$P_j = 2\,\pi\,{d_j}^{-1}$.

Similarly, setting both $b_j$ and $d_j$ to zero, we get a Ornstein--Uhlenbeck
process
\begin{eqnarray}
k_j(\tau_{nm}) = a_j\,\exp\left(-c_j\,\tau_{nm}\right)
\end{eqnarray}
with the PSD
\begin{eqnarray}
S_j(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{a_j}{c_j}\,
    \frac{1}{1+\left(\frac{\omega}{c_j}\right)^2} \quad.
\end{eqnarray}

It's worth noting that the product of two terms of the form found inside the
sum in \eq{celerite-kernel} can also be re-written as a sum with updated
parameters
\begin{eqnarray}\eqlabel{product-rule}
k_j(\tau) \, k_k(\tau) =
    e^{-\tilde{c}\,\tau}\,[
        a_+\,\cos(d_+\,\tau) + b_+\,\sin(d_+\,\tau) +
        a_-\,\cos(d_-\,\tau) + b_-\,\sin(d_-\,\tau)
    ]
\end{eqnarray}
where
\begin{eqnarray}
    \tilde{a}_{\pm} &=& \frac{1}{2}\,(a_j\,a_k \pm b_j\,b_k) \\
    \tilde{b}_{\pm} &=& \frac{1}{2}\,(b_j\,a_k \mp a_j\,b_k) \\
    \tilde{c} &=& c_j + c_k \\
    \tilde{d}_{\pm} &=& d_j \mp d_k \quad.
\end{eqnarray}

\section{Implementation \& performance}

\citet{Rybicki:1995} demonstrated that the inverse of a matrix $K$ where the
elements are given by
\begin{eqnarray}
K_{nm} = k(\tau_{nm})
\end{eqnarray}
for $k(\tau_{nm})$ given by equation \eq{kernel-simple} could be computed
efficiently by taking advantage of the structure of this covariance function
and \citet{Ambikasaran:2015} generalized this computation to apply to the full
mixture of $J$ terms in \eq{celerite-kernel-complex} and derived an equally
efficient method for computing the determinant of $K$.

\subsection{An example}

To provide some insight for this method, we will start by working through a
simple example.
In this case, we'll assume that we have three data points
$\{y_1,\,y_2,\,y_3\}$ observed at times $\{t_1,\,t_2,\,t_3\}$ with measurement
variances $\{{\sigma_1}^2,\,{\sigma_2}^2,\,{\sigma_3}^2\}$ and we would
like to compute the likelihood of these data under a GP model with the
covariance function
\begin{eqnarray}
k(\tau_{nm}) = \sigma_n^2\,\delta_{nm} + a\,\exp(-c\,\tau_{nm})\quad.
\end{eqnarray}
This function is identical to \eq{celerite-kernel-complex} with $J=1$, $b=0$,
and $d=0$.
To demonstrate this method, we will write out the full system of
equations that we must solve to apply the inverse of $K$ in order to compute
the first term of \eq{gp-likelihood}.
In matrix notation, this can be written as
\begin{eqnarray}\eqlabel{impl-matrix}
K\,\bvec{z} &=& \bvec{y}, \\
\begin{pmatrix}
    a+{\sigma_1}^2 & a\,e^{-c\,\tau_{2,1}} & a\,e^{-c\,\tau_{3,1}}\\
    a\,e^{-c\,\tau_{2,1}} & a+{\sigma_2}^2 & a\,e^{-c\,\tau_{3,2}}\\
    a\,e^{-c\,\tau_{3,1}} & a\,e^{-c\,\tau_{3,2}} & a+{\sigma_3}^2
\end{pmatrix}\,
\begin{pmatrix}
    z_1 \\ z_2 \\ z_3
\end{pmatrix} &=&
\begin{pmatrix}
    y_1 \\ y_2 \\ y_3
\end{pmatrix}
\end{eqnarray}
where our goal is to solve for the unknown vector \bvec{z} for a given matrix
$K$ and vector \bvec{y}.
In this equation, we have assumed that the mean function is zero but a
non-zero mean could be included by replacing \bvec{y} by
$\bvec{r}_\bvec{\theta}$ as defined in \sect{gps}.
Now, if we introduce the variables
\begin{eqnarray}\eqlabel{algo-first}
    u_n = e^{-c\,\tau_{n+2,n+1}}\,u_{n+1} + a\,z_{n+1}
\end{eqnarray}
where $u_{N} = 0$, and
\begin{eqnarray}
    g_n = e^{-c\,\tau_{n+1,n}}\,g_{n-1} + e^{-c\,\tau_{n+1,n}}\,z_{n}
\end{eqnarray}
where $g_{0} = 0$,
the system of equations can be rewritten as
\begin{eqnarray}
(a+{\sigma_1}^2)\,z_1 + e^{-c\,\tau_{2,1}}\,u_1 &=& y_1 \\
a\,g_1 + (a+{\sigma_2}^2)\,z_2 + e^{-c\,\tau_{3,2}}\,u_2 &=& y_2 \\
a\,g_2 + (a+{\sigma_3}^2)\,z_3 &=& y_3 \quad. \eqlabel{algo-last}
\end{eqnarray}
The system defined by \eq{algo-first} through \eq{algo-last} can be
rewritten as a matrix equation to show the benefit that this seemingly trivial
reformulation provides:
\begin{eqnarray}
\begin{pmatrix}
    a+{\sigma_1}^2 & e^{-c\,\tau_{2,1}} & 0 & 0 & 0 & 0 & 0 \\
    e^{-c\,\tau_{2,1}} & 0 & -1 & 0 & 0 & 0 & 0 \\
    0 & -1 & 0 & a & e^{-c\,\tau_{3,2}} & 0 & 0 \\
    0 & 0 & a & a+{\sigma_2}^2 & e^{-c\,\tau_{3,2}} & 0 & 0 \\
    0 & 0 & e^{-c\,\tau_{3,2}} & e^{-c\,\tau_{3,2}} & 0 & -1 & 0 \\
    0 & 0 & 0 & 0 & -1 & 0 & a \\
    0 & 0 & 0 & 0 & 0 & a & a+{\sigma_3}^2 \\
\end{pmatrix}\,
\begin{pmatrix}
    z_1 \\ u_1 \\ g_1 \\ z_2 \\ u_2 \\ g_2 \\ z_3
\end{pmatrix} &=&
\begin{pmatrix}
    y_1 \\ 0 \\ 0 \\ y_2 \\ 0 \\ 0 \\ y_3
\end{pmatrix}\nonumber
\end{eqnarray}
where we will follow \citet{Ambikasaran:2015} and call this the ``extended''
system and rewrite \eq{impl-matrix} as
\begin{eqnarray}
    K_\mathrm{ext}\,\bvec{z}_\mathrm{ext} &=& \bvec{y}_\mathrm{ext} \quad.
\end{eqnarray}
Even though $K_\mathrm{ext}$ is a larger matrix than the original $K$ that we
started with, it is now has a sparse banded structure that can be exploited to
solve the system efficiently.
In particular, band matrix solvers are available that can perform an
LU-decomposition of banded matrices like this in $\mathcal{O}(N)$
operations~--~instead of the $\mathcal{O}(N^3)$ that would be required in
general~--~and we can take advantage of these algorithms to solve our system
exactly because the target vector $\bvec{z}$ is a subset of the elements of
$\bvec{z}_\mathrm{ext}$.

In the following section, we will discuss this method more generally but it's
worth noting a few important facts that can already be seen in this example.
First, the fundamental reason why this matrix $K$ can be solved efficiently is
the following property of exponentials
\begin{eqnarray}
    e^{-c\,(t_3 - t_2)} \, e^{-c\,(t_2 - t_1)} =
    e^{-c\,(t_3 - t_2 + t_2 - t_1)} =
    e^{-c\,(t_3 - t_1)}
\end{eqnarray}
and it is important to note that this property does not extend to other common
covariance functions like the ``exponential-squared'' function
\begin{eqnarray}
    k(\tau) \propto e^{-c\,\tau^2} \quad.
\end{eqnarray}
Furthermore, our derivation of the extended matrix requires that the data
points be monotonically sorted in time.
Neither of these properties will be satisfied in general for multidimensional
inputs and all of our following discussion will assume a sorted
one-dimensional dataset.

\citet{Ambikasaran:2015} demonstrated two key facts that allow us to use this
extended matrix formalism in practice.
First, even if the covariance function is a mixture of exponentials, the
extended matrix will still be banded with a bandwidth that scales linearly
with the number of components $J$.
Second, \citet{Ambikasaran:2015} proved that the determinant of
$K_\mathrm{ext}$ is equal to the determinant of $K$ up to a sign.
This means that we can use this extended matrix formalism to compute the
marginalized likelihood in $\mathcal{O}(N)$ operations.

\subsection{The algorithm}

In this section, we generalize the method from the previous section to the
covariance function given by \eq{celerite-kernel}.
This derivation follows \citet{Ambikasaran:2015} but it includes explicit
treatment of complex parameters, and their complex conjugates.

In the case of the full \celerite\ covariance function
(\eqalt{celerite-kernel}), we introduce the following auxiliary variables in
analogy to the $u_n$ and $g_n$ that we introduced in the previous section
\begin{eqnarray}\eqlabel{full-system-first}
\phi_{n,j} &=& e^{-c_j\,\tau_{n+1,n}}\,\cos\left(d_j\,\tau_{n+1,n}\right)\\
\psi_{n,j} &=& -e^{-c_j\,\tau_{n+1,n}}\,\sin\left(d_j\,\tau_{n+1,n}\right)\\
g_{n,j} &=& \phi_{n,j}\,g_{n-1,j} + \phi_{n,j}\,z_n + \psi_{n,j}\,h_{n-1,j}\\
h_{n,j} &=& \phi_{n,j}\,h_{n-1,j} - \psi_{n,j}\,z_n - \psi_{n,j}\,g_{n-1,j}\\
u_{n,j} &=& \phi_{n+1,j}\,u_{n+1,j} + a_j\,z_{n+1} + \psi_{n+1,j}\,v_{n+1,j}\\
v_{n,j} &=& \phi_{n+1,j}\,v_{n+1,j} - b_j\,z_{n+1} - \psi_{n+1,j}\,u_{n+1,j}
\end{eqnarray}
with the boundary conditions
\begin{eqnarray}
    g_{0,j} = 0 \quad, \quad
    h_{0,j} = 0 \quad, \quad
    u_{N,j} = 0 \quad, \quad\mathrm{and}\quad
    v_{N,j} = 0
\end{eqnarray}
for all $j$.
Using these variables and some algebra, we find that the following expression
\begin{eqnarray}\eqlabel{full-system-last}
\sum_{j=1}^J \left[a_j\,g_{n,j}+b_j\,h_{n,j}\right]
+ \left[{\sigma_n}^2+\sum_{j=1}^J a_j\right]
+ \sum_{j=1}^J \left[\phi_{n,j}\,u_{n,j}+\psi_{n,j}\,v_{n,j}\right]
    &=& r_{\bvec{\theta},n}
\end{eqnarray}
is equivalent to the target matrix equation
\begin{eqnarray}
K\,\bvec{z} &=& \bvec{r}_\bvec{\theta}
\end{eqnarray}
if $r_{\bvec{\theta},n}$ is the $n$-th element of the residual vector
$\bvec{r}_\bvec{\theta}$ defined in \sect{gps}.
\eq{full-system-first} through \eq{full-system-last} define a banded matrix
equation in the ``extended'' space and, as before, this can be used to solve
for $K^{-1}\,\bvec{r}_\bvec{\theta}$ and $\det K$ in $\mathcal{O}(N)$
operations.
\Figure{matrix} shows a pictorial representation of the sparsity pattern of
the extended matrix $K_\mathrm{ext}$.
Given that definition of $K_\mathrm{ext}$, the corresponding extended vectors
$\bvec{z}_\mathrm{ext}$ and $\bvec{r}_\mathrm{ext}$ are defined schematically
as
\begin{eqnarray}
{\bvec{z}_\mathrm{ext}} ^\T =
\left(\begin{array}{cccccccccc}
    z_1 & u_{1,j} & v_{1,j} & g_{1,j} & h_{1,j} & z_2 & u_{2,j} & \cdots &
    h_{N-1,j} & z_N
\end{array}\right)
\end{eqnarray}
and
\begin{eqnarray}
{\bvec{r}_\mathrm{ext}} ^\T =
\left(\begin{array}{cccccccccc}
    r_{\bvec{\theta},1} & 0 & 0 & 0 & 0 & r_{\bvec{\theta},2} &
    0 & \cdots & 0 & r_{\bvec{\theta},2}
\end{array}\right) \quad.
\end{eqnarray}

After constructing the extended matrix (using a compact storage format), the
extended matrix can be factorized using a LU-decomposition\footnote{Even
though $K_\mathrm{ext}$ is symmetric, it is not positive definite so a
Cholesky solver cannot be used for increased efficiency.} routine optimized
for band matrices.
This decomposition can be used to compute the determinant of $K$, solve
$K^{-1}\,\bvec{r}_\bvec{\theta}$, and subsequently calculate the marginalized
likelihood in \eq{gp-likelihood}.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/matrix.pdf}
\caption{A pictorial representation of the sparse extended matrix
$K_\mathrm{ext}$ with $N=5$ and $J=2$.
Each colored block corresponds to a non-zero entry in the matrix as described
in the legend.
    \figurelabel{matrix}}
\end{center}
\end{figure}


\subsection{Implementation considerations \& scaling}

The extended system defined in the previous section is sparse with fewer than
a few percent non-zero entries and band structure.
In this \sectionname, we empirically investigate the performance and scaling
of three different algorithms for solving this extended systems:

\begin{enumerate}

\item A simple algorithm for computing the LU decomposition for banded
    matrices using Gaussian elimination \citep{Press:1992, Press:2007},

\item The general banded LU decomposition implementation from
    LAPACK\footnote{We use the \texttt{dgbtrf} and \texttt{dgbtrs} methods
        from LAPACK.} \citep{Anderson:1999} using optimized BLAS routines, and

\item A general sparse LU solver that exploits the sparsity but not the band
    structure \citep{Guennebaud:2010}.

\end{enumerate}

NNZ: $(10\,J + 1)\,N - 4\,J + 1$

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{../examples/benchmark/benchmark_darwin_default.pdf}
\caption{Benchmark\figurelabel{benchmark}}
\end{center}
\end{figure}

\section{celerite as a model of stellar variations}

\dfmtodo{Add more intro to this section.}

A special case of the \celerite\ model of great physical interest is a
stochastically-driven simple harmonic oscillator.
The differential equation for this system is
\begin{equation}
    \left[\frac{\dd^2}{\dd t^2} + \frac{\omega_0}{Q}\,\frac{\dd}{\dd t}
    + \omega_0^2\right]\, y(t) = \epsilon(t)
\end{equation}
where $\omega_0$ is the frequency of the undamped oscillator, $Q$ is the
quality factor of the oscillator, and $\epsilon(t)$ is a stochastic driving
force.
In the limit of an infinite time series and white random forcing, the PSD of
this equation is given by \citep{Anderson:1990}
\begin{equation}\eqlabel{sho-psd}
S(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{S_0\,\omega_0^4}
    {(\omega^2-\omega_0^2)^2 + \omega_0^2\omega^2/Q^2}
\end{equation}
where $S_0$ is proportional to the power at $\omega = \omega_0$, $S(\omega_0)
= \sqrt{2/\pi}\,S_0\,Q^2$.
The power spectrum in \eq{sho-psd} matches \eq{celerite-psd} if
\begin{eqnarray}\eqlabel{sho-complex}
a_j &=& S_0\,\omega_0\,Q \\
b_j &=& \frac{S_0\,\omega_0\,Q}{\sqrt{4\,Q^2-1}} \\
c_j &=& \frac{\omega_0}{2\,Q}\\
d_j &=& \frac{\omega_0}{2\,Q} \sqrt{4\,Q^2-1} \quad,
\end{eqnarray}
for $Q \ge \frac{1}{2}$.
For $0 < Q \le \frac{1}{2}$, \eq{sho-psd} can be captured by a pair of \celerite\
terms with parameters
\begin{eqnarray}\eqlabel{sho-real}
a_{j\pm} &=& \frac{1}{2}\,S_0\,\omega_0\,Q\,\left[ 1 \pm
        \frac{1}{\sqrt{1-4\,Q^2}}\right] \\
b_{j\pm} &=& 0 \nonumber\\
    c_{j\pm} &=& \frac{\omega_0}{2\,Q}\,\left[1 \mp \sqrt{1-4\,Q^2}\right]
    \nonumber\\
d_{j\pm} &=& 0 \quad. \nonumber
\end{eqnarray}

These identities yield a kernel of the form
\begin{equation}\eqlabel{sho-kernel}
k(\tau) = S_0\,\omega_0\,Q\,e^{-\frac{\omega_0\,\tau}{2Q}}\,
\begin{cases}
    \cosh{(\eta\,\omega_0\,\tau)} +
        \frac{1}{2\,\eta\,Q}\,\sinh{(\eta\,\omega_0\,\tau)}, & 0 < Q < 1/2\\
    2\,(1+\omega_0\,\tau), & Q = 1/2\\
    \cos{(\eta\,\omega_0\,\tau)} +
        \frac{1}{2\,\eta\,Q} \sin{(\eta\,\omega_0\,\tau)},& 1/2 < Q\\
\end{cases}
\end{equation}
where $\eta = \vert 1-(4\,Q^2)^{-1}\vert^{1/2}$.
It is interesting to note that, because of the damping, the characteristic
oscillation frequency in this model $d_j$, for any finite quality factor $Q > 1/2$,
is not equal to the frequency of the undamped oscillator $\omega_0$.

The power spectrum in \eq{sho-psd} has several limits of physical interest:
\begin{itemize}

{\item For $Q = 1/\sqrt{2}$, \eq{sho-psd} simplifies to
\begin{eqnarray}\eqlabel{granulation-psd}
S(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{S_0}{(\omega/\omega_0)^4+1} \quad.
\end{eqnarray}
This, in turn, is the most commonly used model for the background granulation
noise in asteoreseismic \citep{Kallinger:2014} and helioseismic
\citep{Harvey:1985, Michel:2009} analyses.
The Fourier transform of this PSD corresponds to the kernel
\begin{equation}
k(\tau) = S_0\,\omega_0\,e^{-\frac{1}{\sqrt{2}}\,\omega_0\,\tau}\,
    \cos{\left(\frac{\omega_0\,\tau}{\sqrt{2}}-\frac{\pi}{4}\right)}.
\end{equation}}

{\item Substituting $Q = 1/2$, \eq{sho-psd} becomes
\begin{eqnarray}
S(\omega) =
    \sqrt{\frac{2}{\pi}}\,\frac{S_0}{\left[(\omega/\omega_0)^2+1\right]^2}
\end{eqnarray}
with the corresponding covariance function (using
\eqalt{celerite-kernel} and \eqalt{sho-real})
\begin{eqnarray}\eqlabel{approx-matern}
k(\tau) &=& \lim_{f \to 0}\,
    \frac{1}{2}\,S_0\,\omega_0\,
    \left[\left(1+1/f\right)\,e^{-\omega_0\,(1-f)\,\tau} +
          \left(1-1/f\right)\,e^{-\omega_0\,(1+f)\,\tau}
    \right] \\
&=& S_0\,\omega_0\,e^{-\omega_0\,\tau}\,[1+\omega_0\,\tau]
\end{eqnarray}
or equivalently (using \eqalt{celerite-kernel} and \eqalt{sho-complex})
\begin{eqnarray}\eqlabel{approx-matern}
k(\tau) &=& \lim_{f \to 0}\,
    S_0\,\omega_0\,e^{-\omega_0\,\tau}\,
    \left[\cos(f\,\tau) + \frac{\omega_0}{f}\,\sin(f\,\tau)\right] \\
&=& S_0\,\omega_0\,e^{-\omega_0\,\tau}\,[1+\omega_0\,\tau] \quad.
\end{eqnarray}
This covariance function is also known as the Mat\'ern-3/2 function
\citep{Rasmussen:2006}.
This suggests that the Mat\'ern-3/2 covariance can be well approximated using
the \celerite\ framework with a small value of $f$ in \eq{approx-matern} but we
caution that this could also lead to numerical issues with the solver.
}

{\item Finally, in the limit of large $Q$, the model approaches a high
    quality oscillation with frequency $\omega_0$ and covariance function
\begin{eqnarray}
k(\tau) \approx
    S_0\,\omega_0\,Q\,
    \exp\left(-\frac{\omega_0\,\tau}{2\,Q}\right)\,
    \cos\left(\omega_0\,\tau\right) \quad.
\end{eqnarray}}

\end{itemize}
\Figure{sho} shows a plot of the PSD for these limits and several other values
of $Q$.
This figure demonstrates that for $Q \le 1/2$, the model has no oscillatory
behavior and that for large $Q$, the shape of the PSD near the peak frequency
approaches a Lorentzian.

These special cases demonstrate that the stochastically-driven simple harmonic
oscillator provides a physically motivated model that is flexible enough to
describe a wide range of stellar variations.
Low $Q \approx 1$ can capture granulation noise and high $Q \gg 1$ is a good
model for asteroseismic oscillations.
In practice, we will take a sum over oscillators with different values of $Q$,
$S_0$, and $\omega_0$ to give a sufficient accounting of the power spectrum
stellar time series.
Since this kernel is exactly described by the exponential kernel, the
likelihood (\eqalt{gp-likelihood}) can be evaluated for a time series with $N$
measurements in $\mathcal{O}(N)$ operations using the \celerite\ method
described in the previous section.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/sho.pdf}
\caption{(left) The power spectrum of a stochastically-driven simple harmonic
    oscillator (\eqalt{sho-psd}) plotted for several values of the quality
    factor $Q$.
    For comparison, the dashed line shows the Lorentzian function from
    \eq{lorentz-psd} with $c_j = \omega_0/2\,Q = 1/20$ and normalized so that
    $S(d_j)/S(0) = 100$.
    (right) The corresponding autocorrelation functions with the same colors.
    \figurelabel{sho}}
\end{center}
\end{figure}


\section{Examples with simulated data}

\subsection{Recovery of a celerite process}

In this first example, we will simulate a dataset using a known \celerite\
process and fit it with \celerite\ to demonstrate that valid inferences can be
made in this idealized case.
The simulated dataset is shown in the left panel of \Figure{simulated-correct}
and it was generated using a SHO kernel (\eqalt{sho-kernel}) with parameters
$S_0 = 1$, $\omega_0 = e^2$, and $Q = e^2$.
The true PSD is shown as a dashed line in the right panel of
\Figure{simulated-correct}.
We applied log-uniform priors to all of the parameters and used \emcee\
\citep{Foreman-Mackey:2013} to sample the joint posterior density and computed
the marginalized posterior inference of the PSD.
This inference is shown in the right panel of \Figure{simulated-correct} as a
blue contour indicating 68\% of the posterior mass.
It is clear from this figure that, as expected, the inference correctly
reproduces the true PSD.


\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/simulated/correct.pdf}
\caption{(left) A simulated dataset.
    (right) The inferred PSD~--~the blue contours encompass 68\% of the
    posterior mass~--~compared to the true PSD (dashed black line).
    \figurelabel{simulated-correct}}
\end{center}
\end{figure}


\subsection{Inferences with the ``wrong'' model}

For this example, we simulate a dataset using a known GP model with a kernel
outside of the support of a \celerite\ process.
This means that the true autocorrelation of the process can never be correctly
represented by the model that we're using to fit but we will use this example
to demonstrate that, at least in this case, valid inferences can still be made
about the physical parameters of the model.

For this example, the data are simulated from a quasiperiodic GP with the
kernel
\begin{eqnarray}\eqlabel{sim-wrong-true}
k_\mathrm{true} (\tau) = \alpha\,
    \exp\left(-\frac{\tau^2}{2\,\lambda^2}\right)\,
    \cos\left(\frac{2\,\pi\,\tau}{P_\mathrm{true}}\right)
\end{eqnarray}
where $P_\mathrm{true}$ is the fundamental period of the process.
This autocorrelation structure yields the power spectrum
\begin{eqnarray}
S_\mathrm{true} (\omega) = \frac{\lambda\,\alpha}{2}\,\left[
    \exp\left(-\frac{\lambda^2}{2}\,\left(\omega-
        \frac{2\,\pi}{P_\mathrm{true}}\right)^2\right) +
    \exp\left(-\frac{\lambda^2}{2}\,\left(\omega+
        \frac{2\,\pi}{P_\mathrm{true}}\right)^2\right)
\right]
\end{eqnarray}
that for large $\omega$ falls off exponentially.
When compared to \eq{celerite-psd}~--~that for large $\omega$ goes as
$\omega^{-4}$, at most~--~it is clear that a \celerite\ model can never
perfectly reproduce the structure of this process.
That being said, we will demonstrate that rigorous inferences can be made
about $P_\mathrm{true}$ even with an effective model.
The left panel of \Figure{simulated-wrong} shows the simulated dataset.
We then fit this simulated data using the product of two SHO terms
(\eqalt{sho-kernel}) where one of the terms has $S_0 = 1$ and $Q =
1/\sqrt{2}$ and the other has $\omega_0 = 2\,\pi/P$.
We note that using \eq{product-rule}, the product of two \celerite\ terms can
also be expressed as a \celerite\ term.
We apply log-uniform priors on all the parameters and use \emcee\
\citep{Foreman-Mackey:2013} to sample the posterior probability for all of the
parameters.
The inferred distribution for the parameter $P$ is shown in the right panel of
\Figure{simulated-wrong} and compared to the true period $P_\mathrm{true}$ and
the inferences made using the correct model (\eqalt{sim-wrong-true}).
The inference made using this effective \celerite\ model are indistinguishable
from the inferences made using the correct model but substantially less
computation time is required for the \celerite\ inference.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/simulated/wrong-qpo.pdf}
\caption{(left) A simulated dataset.
    (right) The inferred period of the process. The true period is indicated
    by the vertical orange line, the posterior inference using the correct
    model is shown as the blue dashed histogram, and the inference made using
    the ``wrong'' effective model is shown as the black histogram.
    \figurelabel{simulated-wrong}}
\end{center}
\end{figure}


\section{Examples with real data}

In this section we will demonstrate several use cases of \celerite\ when
applied to real datasets.
Each of these examples touches on an active area of research so we will limit
our examples to be qualitative in nature and make no claim of optimality but
we hope that these examples will encourage interested readers to investigate
the applicability of \celerite\ to their research.

All of the following examples show time domain datasets with a clear bias in
favor of large homogeneous photometric surveys but these methods can similarly
be applied to spectroscopy, where wavelength~--~instead of time~--~is the
independent coordinate and other one-dimensional domains.
It is also possible to cast some two-dimensional problems in the \celerite\
framework (\dfmtodo{CITE Ian C}).

\subsection{Asteroseismic oscillations}

The asteroseismic oscillations of thousands of stars were measured using light
curves from the \kepler\ mission (\dfmtodo{cite}) and asteroseismology is a
key science driver for many of the upcoming large scale photometric surveys
(\dfmtodo{cite}).
Most asteroseismic analyses have been limited to relatively high
signal-to-noise oscillations because the standard methods based on statistics
of the empirical periodogram of the data cannot be used to formally propagate
the measurement uncertainties to the constraints on physical parameters
(\dfmtodo{CITE}) and more sophisticated methods are computationally expensive
and they scale poorly to the current state-of-the-art datasets (\dfmtodo{CITE
Brewer etc.}).

\celerite\ alleviates these problems by providing a physically motivated
probabilistic model that can be evaluated efficiently even for large datasets.
In practice, we will model the star as a mixture of stochastically driven
simple harmonic oscillators where the amplitudes and frequencies of the
oscillations are computed using a physical model and evaluate the probability
of the observed dataset using a Gaussian Process with a PSD given by a sum of
terms given by \eq{sho-psd}.
This gives us a method of computing the likelihood function for the parameters
of the physical model in $\mathcal{O}(N)$ operations and this can be combined
with standard non-linear optimization or posterior sampling methods to infer
constraints on the parameters for the given dataset.

To demonstrate this method, we will use a very simple heuristic model based on
\dfmtodo{CITE} where the model PSD is given by a mixture of 8 components with
amplitudes and frequencies specified by $\nu_\mathrm{max}$, $\Delta \nu$, and
several nuisance parameters.
The first term is used to capture the granulation ``background'' using
\eq{granulation-psd} with two free parameters $S_g$ and $\omega_g$.
The remaining  7 terms are given by \eq{sho-psd} where $Q$ is a nuisance
parameter shared between terms and the frequencies are given by
\begin{eqnarray}
\omega_{0,\,j} = 2\,\pi\,(\nu_\mathrm{max} + j\,\Delta\nu + \epsilon)
\end{eqnarray}
and the amplitudes are given by
\begin{eqnarray}
S_{0,\,j} =
    \frac{A}{Q^2}\,\exp\left(-\frac{[j\,\Delta\nu + \epsilon]^2}{2\,W^2}\right)
\end{eqnarray}
where $j$ is an integer running from $-3$ to 3 and $\epsilon$, $A$, and $W$
are shared nuisance parameters.
This model could be easily extended to include small frequency splitting and
$\nu_\mathrm{max}$ and $\Delta \nu$ could be replaced by physical parameters
like $\log g$.

To demonstrate the applicability of this model, we apply it to infer the
asteroseismic parameters of the giant star KIC~11615890 observed by the
\kepler\ Mission.
The goal of this example is to show that, even for a low signal-to-noise
dataset with a short baseline, it is possible to infer asteroseismic
parameters with formal uncertainties that are consistent with the parameters
inferred with a much larger dataset.
Looking forward to \tess, we will infer $\nu_\mathrm{max}$ and $\Delta\nu$
using only one month of \kepler\ data and compare our results to the results
inferred from the full 4 year baseline of the mission.
For KIC~11615890, the published asteroseismic parameters based on the full
dataset are \citep{Pinsonneault:2014}
\begin{eqnarray}
    \nu_\mathrm{max} = 171.94 \pm 3.62 \,\mu\mathrm{Hz} \quad\mathrm{and}\quad
    \Delta\nu = 13.28 \pm 0.29 \,\mu\mathrm{Hz} \quad.
\end{eqnarray}
We randomly select a month-long segment of \kepler\ data, initialize our
\celerite\ model using a grid search in the parameter space, and then use
\emcee\ \citep{Foreman-Mackey:2013} to sample the joint posterior density for
the full set of parameters.
\Figure{astero-corner} shows the marginalized density for $\nu_\mathrm{max}$
and $\Delta\nu$ compared to the results from the literature.
\dfmtodo{Add some more discussion here.}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/astero-11615890-numax_deltanu_corner.pdf}
\caption{The probabilistic constraints on $\nu_\mathrm{max}$ and $\Delta \nu$
    from the inference shown in \Figure{astero} compared to the published
    value (error bar) based on the full \kepler\ dataset \dfmtodo{CITE}.
    \figurelabel{astero-corner}}
\end{center}
\end{figure}


\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/astero-11615890-comparisons.pdf}
\caption{A comparison between the empirical PSD and the posterior inference of
the PSD as a mixture of stochastically driven simple harmonic oscillators.
(top) The periodogram of the \kepler\ light curve for KIC~11615890 computed
    on the full four year baseline of the mission.
    The light gray curve shows the raw periodogram and the black curve has
    been smoothed with a Gaussian filter with \dfmtodo{SOME WIDTH}.
(middle) The same periodogram computed using about a month of data.
(bottom) The power spectrum inferred using the mixture of SHOs model described
    in the text and only one month of \kepler\ data.
    The black line shows the median of posterior PSD and the gray contours
    show the 68\% credible region.
    \figurelabel{astero}}
\end{center}
\end{figure}

\subsection{Stellar rotation}

Another source of variability that can be measured from time series
measurements of stars is rotation.
The inhomogeneous surface of the star (spots, plage, \etc) imprints itself as
quasiperiodic variations in photometric or spectroscopic observations
(\dfmtodo{CITE}).
It has been demonstrated that for light curves with nearly uniform sampling,
the empirical autocorrelation function provides a reliable estimate of the
rotation period of a star (\dfmtodo{CITE}) and that a GP model with a
quasiperiodic covariance function can be used to make probabilistic
measurements even with sparsely sampled data (R.~Angus, \etal\ in prep.).
The covariance function used for this type of analysis has the form
\begin{eqnarray}\eqlabel{sine2}
k(\tau) = A\,\exp\left(-\frac{\tau^2}{2\,\ell^2} -
    \Gamma\,\sin^2\left(\frac{\pi\,\tau}{P} \right) \right)
\end{eqnarray}
where $P$ is the period of the oscillation.
The key difference between this function and other quasiperiodic kernels is
that it is everywhere positive.
We can construct a simple \celerite\ covariance function with similar properties
as follows
\begin{eqnarray}\eqlabel{rot-kernel}
k(\tau) = \frac{a}{2+b}\,e^{-c\,\tau}\,\left[
    \cos\left(\frac{2\,\pi\,\tau}{P}\right) + (1 + b)
\right]
\end{eqnarray}
for $a>0$, $b>0$, and $c>0$.
The covariance function in \eq{rot-kernel} cannot exactly reproduce \eq{sine2}
but, since \eq{sine2} is only an effective model, \eq{rot-kernel} can be used
as a drop-in replacement for a substantial gain in computational efficiency.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/rotation.pdf}
\caption{Inferred constraints on a quasiperiodic GP model using the covariance
    function in \eq{rot-kernel} and one quarter of \kepler\ data.
(left) The \kepler\ data (black points) and the maximum likelihood model
    prediction (blue curve).
    The solid blue line shows the predictive mean and the blue contours show
    the predictive standard deviation.
(center) Inferred constraints on the model PSD.
    The dashed line shows the maximum likelihood PSD, the blue solid line
    shows the median of posterior PSD, and the blue contours show the 68\%
    credible region.
(right) Inferred constraints on the model covariance function from
    \eq{rot-kernel}.
    The dashed line shows the maximum likelihood model, the blue solid line
    shows the median of posterior, and the blue contours show the 68\%
    credible region.
    \figurelabel{rotation}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/rotation-period.pdf}
\caption{The posterior constraint on the rotation period of KIC~1430163 using
    the dataset and model from \Figure{rotation}.
    The period is the parameter $P$ in \eq{rot-kernel} and this figure shows
    the posterior distribution marginalized over all other nuisance parameters
    in \eq{rot-kernel}.
    This is consistent with the published rotation period made using the
    autocorrelation function and the full \kepler\ data (\dfmtodo{CITE}).
    \figurelabel{rotation-period}}
\end{center}
\end{figure}

\subsection{Exoplanet transit fitting}

In this example, we inject the signal of a simulated exoplanet transit into a
real \kepler\ light curve and then demonstrate that we can recover the true
physical parameters of the exoplanet while modeling the stellar variability
using \celerite.
This example is different from all the previous examples because in this case,
we are uninterested in the inferred parameters of the covariance model.
Instead, we're interested in inferring constraints on the parameters of the
mean model.
In \eq{gp-likelihood} these parameters are called $\bvec{\theta}$ and in this
example, the mean function $\mu_\bvec{\theta}(t)$ is a limb-darkened transit
light curve \dfmtodo{CITE MA} parameterized by a period $P$, a transit
duration $T$, a phase or epoch $t_0$, an impact parameter $b$, the radius
of the planet in units of the stellar radius $R_P/R_\star$, and several
parameters describing the limb-darkening profile of the star \dfmtodo{CITE}.

KIC~1430163 with simulated transit.
\Figure{transit-ml} and \Figure{transit-corner}.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/transit-ml.pdf}
\caption{\figurelabel{transit-ml}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/transit-corner.pdf}
\caption{\figurelabel{transit-corner}}
\end{center}
\end{figure}

\section{Comparisons to other methods}

Toeplitz, KISS-GP, CARMA, HODLR.

Limitations of celerite: one-dimension, stationary, etc.


\section{Summary}



% DFM: I'm going to redistribute the following in other sections.

% Stars are variable, for better or worse:  their variability provides information about the properties
% of stars, but also results in noise that must be accounted for in detecting and characterizing their
% planetary systems.  When analyzing astronomical times series of stars, then, there are generally two
% goals:  1). to analyze the frequency spectrum and amplitude of stellar variability to better understand stellar
% properties and evolution, and/or 2). to account for correlated noise when modeling the stellar
% flux variability and spectral variations.

% A promising technique for addressing both of these goals is the Gaussian Process formalism
% \citep{Rasmussen2006,Gibson:2012}.  The basis of this method is to assume that stellar variability
% is random, Gaussian noise, but that the random variability is correlated in time, and the nature of the
% correlations is completely specified by a correlation matrix.  In its simplest form, the
% correlation matrix is modeled with a time-independent autocorrelation function (although
% the time-independence can be relaxed), including a white-noise component that may vary
% with each exposure, and the autocorrelation function is described by a functional form that may be
% simply parameterized by a relatively small number of parameters, sometimes referred to as
% `hyper-parameters.'  The goal of spectral analysis is to measure the parameters which
% describe the autocorrelation function, thus inferring properties of the stellar variabilty
% which may be used to characterize properties of the star, such as the rotation period,
% the asteroseismic frequencies, or granulation noise known as `flicker'
% \citep{Aerts2010,Noyes:1984,Bastien:2013}.
% The goals of planet detection include measuring the Doppler shift of the star, measuring
% the decrement of flux as a planet transits a star, and/or measuring the times of transit to
% look for dynamical interactions between planets.  To properly account for the impact of
% stellar variability on the planet parameters requires treating correlations in the data, and
% to do so also requires evaluating the correlation matrix.

% The size of the correlation matrix, $N_{elements}$, scales as the square of the length of the time-series,
% $N_{element}=N_{time}^2$, so as time series grow in size, say $N_{time} =10^{5-6}$, the storage may become
% prohibitive, $N_{element} = 10^{10-12}$.  In addition, this matrix must be inverted and have
% its determinant evaluated to compute the likelihood function, which takes of order $N_{operations}
% = N_{time}^3 = 10^{15-18}$.  This becomes an impossible computational problem, which prohibits
% using long time series datsets, such as the {\it Kepler} dataset $N_{time} \approx 10^{5-6}$,
% {\it Spitzer} time series with $N_{time} \approx 10^5$, and Solar time series with
% $N_{time} \approx 10^7$.  So a faster means of solving for the likelihood is required in these
% cases.

% One promising solution is to use approximate techiques based upon the fact that correlation
% matrices display a great deal of symmetry.  This `hierarchical off-diagonal low-rank' (\texttt{HODLR})
% algorithm can make the storage and operations both scale in proportion to $N_{time}$, although the
% solver is only approximate \citep{Ambikasaran:2013,Ambikasaran:2016}.  In addition, the
% solver is complicated and can be challenging to use with general choices of kernel.  Another
% promising approximate technique is to use a uniform resampling of the dataset, and then
% rely on the structure of Toeplitz matrices, which require uniformly space time series,
% to obtain approximate solutions of the likelihood function \citep{Wilson:2015},
% the `Kernel Interpolation for Scalable Structured Gaussian Processes' (\texttt{KISS-GP});
% this approach has the drawback of being approximate.  A third approach is to use a kernel
% based upon stochastic differential equations:  the `continuous auto-regressive moving-average'
% model \citep[aka \texttt{CARMA};][]{Kelly:2014}.  This approach can model a stationary power-spectrum that
% is the combination of a number of Lorentzians, but has the disadvantage that the amplitudes
% of the Lorentzians are required to have a specific relation.  In addition, these
% techniques require a stationary kernel. {\color{red} Is this true for GEORGE?}

% The approach we explore in this paper is based upon a method developed by Press \&
% Rybicki twenty years ago \citep{Rybicki:1995}.  They observed that a correlation
% matrix consisting of a single exponential kernel has an inverse that is tri-diagonal.
% The coefficients and solution of a tri-diagonal matrix scale in proportion to $O(N_{time})$.
% They also indicated that a combination of two exponential kernels would have a simliar scaling,
% but they did not demonstrate how to extend this to more than two kernel components.

% A significant advance was made in generalizing the Press-Rybicki method to an
% arbitrary number of exponential kernels by utilizing the fact that the elements of
% such matrices can be written in terms of the products of components of two vectors,
% which are so-called semi-separable matrices \citep{Ambikasaran:2015}.  This `Generalized
% Rybicki-Press' (\texttt{celerite}) approach
% has many advantages:  the computation time and storage both scale as $O(N_{time})$,
% the coefficients of the kernels need not be stationary, the computation of the
% likelihood is exact, and the algorithm is simple to implement, and thus is robust.
% Unlike the \texttt{CARMA} model, there is no restriction on the relation between
% the coefficients (except that the kernel needs to be positive definite).  Unlike
% \texttt{HODLR} and \texttt{KISS-GP},  the \texttt{celerite} method is exact.  The main drawback
% of this approach is that the sum of exponential kernels with real coefficients can
% only represent a limited range of power spectra of auto-correlated time series:
% those which can be expressed the sum of Lorentzians that peak at zero frequency.

% However, this drawback turns out to not be significant:  the exponential kernel
% can also have a complex exponential coefficients (plus their complex-conjugates), and
% thus approximate an almost arbitrary autocorrelation function with enough terms, most
% importantly allowing for quasi-periodic kernels.  It turns out that the sum of Lorentzian
% components is a {\it very} good approximation of stellar variability spectra:  the
% activity and granulation noise due to stellar convection may be expressed in terms of zero-frequency
% Lorentzians \citep{Harvey:1985}, the asteroseismic oscillations may be expressed as non-zero frequency
% Lorentzians \citep{1990ApJ...364..699A,Gruberbauer:2009}, and stellar rotation may be
% parameterized this way as well.  In this paper
% we extend the \texttt{celerite} approach to kernels with complex exponentials, which when
% combined with the complex conjugate leads to damped harmonic auto-correlation functions
% which are flexible enough to describe a wide range of variability, and may still
% be solved in $O(N_{time}$ operations.

% We show that the extended matrix that embeds this multi-component kernel can be written
% in real notation, which speeds up the evaluation of the likelihood function,
% and allows optimization software to use automatic differential in computing
% derivatives of the likelihood function.

\section{Summary}

Although we have in mind application of this fast method to stellar variability,
the method is general for one-dimensional GP problems, and may be applied to
other problems.    Within astrophysics, correlated noise (due to the environment,
detector, or modeling uncertainty) may be present in
gravitational wave time series, and so Gaussian processes may be a way to address
this problem \citep{Moore:2016}.  Accreting black holes show time series which
may be modeled by correlated noise \citep{Kelly:2014};  indeed, this was the
motivation for the original technique developed by Rybicki \& Press \citep{Rybicki:1992,Rybicki:1995}.
This approach may be broadly used for characterizing quasar variability
\citep{MacLeod:2010}, measuring time lags with reverberation mapping
\citep{Zu:2011}, and modeling time delays in multiply-imaged
gravitationally-lensed systems \citep{Press:1998}.

Outside of astronomy, this technique may have application to seismology
\citep{Robinson:1967},

There are three primary applications we envision for the \celerite\ formalism: 1).\ modeling
the variability of an astrophysical system to infer it properties; 2).\
accounting for astrophysical variability as a source of noise when trying to detect
additional phenomena; 3).\ interpolating or extrapolating variability to future or missing
times.  Some examples of the first are determining the characteristic
variability timescale of a quasar, measuring the asteroseismic variation of a star,
or detection of quasi-periodic variability in a high-energy source.  Examples of the
second are detecting transiting exoplanets and correcting for stellar activity in
radial velocity measurements. Examples of the third are measuring time delays in
multiply-imaged gravitationally lensed sources and reverberation mapping of AGN.

Our background is in studying transiting exoplanets, a field which has only recently begun
to adopt full covariance matrices in analyzing the noise in transiting planet lightcurves.
One promising early attempt at accounting for correlated noise was the wavelet approach
of Carter \& Winn (2010).  Their technique allowed for a power-law spectrum for the
noise scaling as $\omega^{-\alpha}$, which unfortunately is not a normalizable, nor
physically-accurate, description of stellar variability.  Their approach runs in
${\cal O}(N)$ time for $\alpha=1$, which is `flicker' or pink noise;  however, this
particular type of noise overpredicts the power spectrum at low frequency compared
to the granulation power spectrum.  It also cannot describe quasi-periodic noise.

Further progress was made by parameterizing the covariance matrix with simple,
analytic functions that describe the autocorrelation function.  The parameters
of these functions can then be optimized to best match the observed variability
pattern of the residuals (after subtracting a transit model).  The disadvantage of
this approach is that the functions used are chosen somewhat arbitrarily (e.g.\
the exponential-squared function), and the Cholesky decomposition of the covariance
matrix for computing the likelihood takes ${\cal O}(N^3)$ operations which prohibits
application of this technique to large datasets.

Given the drawbacks of these approaches, the \celerite\ formalism allows both a fast
computation of the likelihood in ${\cal O}(N)$ time, as well as a functional form
that accurately describes stellar variability due to the relation to the simple
harmonic oscillator power spectrum, which is an analog of stellar asteroseismic
oscillations.  As higher signal-to-noise observations of transiting exoplanet systems
are obtained, the effects of stellar variability will more dramatically impact the
correct inference of planetary transit parameters, and so we expect that \celerite\
will be important for transit timing, transit spectroscopy, Doppler beaming,
phase functions, and more.

%\section{Prediction/interpolation}
%
%Precition/interpolation involves constructing a matrix of covariances
%between the (noisy) training set and the times at which we wish to
%predict or interpolate data points.   If our training times are given by
%${\bf t}$, while the predicted times are given by ${\bf t}_*$, then
%mean values at the predicted times are given by (RW 2.23):
%\begin{eqnarray}
%\bar f_{*,k} &=& \sum_j k(t_{*,k},t_j) b_j,\\
%&=& \sum_j \sum_p \alpha_p e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
%&=& \sum_p \alpha_p \sum_j e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
%y_k &=& \sum_j k(t_j,t_k) h_j,
%\end{eqnarray}
%where the latter equation may be solved for ${\bf h}$ using the extended
%matrix formalism.  Let the length of ${\bf t}_*$ be $M$; then, the
%matrix $k({\bf t}_*,{\bf t})$ has a size $M \times N$, and so a total
%of $MN$ multiplications is required to obtain the predicted values.
%If $M$ is large, this can become quickly prohibitive.  It turns out
%that the structure of the sum of exponential kernels may be exploited
%to obtain the predicted mean in of order $J(M+N)+J^2N$ multiplications,
%i.e.\ a linear scaling with the total number of data points, which can
%be much fewer if $M$ is large.
%
%The procedure works as follows:
%\begin{itemize}
%\item Sort ${\bf t}$ and ${\bf t}_*$ in time order.
%\item Starting with the smallest value of ${\bf t}_*$, $t_{*,1}$,
%compute:
%\begin{eqnarray} \label{prediction}
%\bar f_{*,k} &=& \sum_p \alpha_p \left[\epsilon^+_{p,k} + \epsilon^-_{p,k}\right]\\
%\epsilon^+_{p,k} &=& \sum_{j \forall t_{*,k} > t_j} e^{-\beta_p (t_{*,k}-t_j)} h_j\\
%\epsilon^-_{p,k} &=& \sum_{j \forall t_{*,k} \le t_j} e^{-\beta_p (t_j-t_{*,k})} h_j,
%\end{eqnarray}
%where we have separated out the training times before and after the predicted time $t_{*,k}$.
%
%\item Next, we update the coefficients recursively:
%\begin{eqnarray}
%\epsilon^+_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^+_{p,k-1} + \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_{*,k}-t_j)} h_j\right]\\
%\epsilon^-_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^-_{p,k-1} - \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_j-t_{*,k})} h_j\right].
%\end{eqnarray}
%With these updates, we can then use equation \ref{prediction} to compute the expected
%mean at $t_{*,k}$ with an additional $J$ operations.  Since there are $N$ values of
%$t_k$, the number of operations needed to update $\epsilon$ is of order $J(N+M)$, and
%so this procedure avoids a heavy computational burden.
%
%The variance may be handled in a similar manner, but with $J^2$ coefficients
%to recursively update. For example, if $t_{*,k} > t_j \forall j,k$ then:
%\begin{eqnarray}
%cov(f_{*,k}) &=&  k(t_{*,k},t_{*,k}) - \sum_p \sum_q \alpha_p \alpha_q e^{-\beta_p(t_{*,k}-{\bf t}^T)} k({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
%&=& k(0) - \sum_p \sum_q \alpha_p \alpha_q \zeta_{p,q,k},\\
%\zeta_{p,q,k} &=&   e^{-\beta_p(t_{*,k}-{\bf t}^T)} k({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
%&=&  e^{-(\beta_p+\beta_q)(t_{*,k}-t_{*,k-1})} \zeta_{p,q,k-1}.
%\end{eqnarray}
%The recursive updates to the $\zeta_{p,q,k}$ parameters can be made at each step.
%\end{itemize}
%
%Problem:  if we have the case that the datapoints are interspersed, and if the sign changes,
%then we need to resolve $K^{-1} e^{-\beta_q \vert t_{*,k}-{\bf t}\vert}$.  However, maybe we can
%just solve for the difference in fewer operations? {\bf TBD}


\appendix

\section{Ensuring positive definiteness: Sturm's theorem}

The power spectrum is computed from taking the square of the Fourier transform
of the data time series; hence, the power-spectrum must be non-negative.  In
constructing a kernel, if any of the parameters $a_j$ or $b_j$ are negative,
then it is possible for the power spectrum to go negative, which violates the
definition of the power spectrum.
Consequently, if any of these coefficients is negative, it is necessary to
check that the power spectrum is still non-negative.  Note that a non-negative
power-spectrum does not require the auto-correlation
function to be positive for all $\tau$.  The positive power spectrum is
related to the positive eigenvalues of the covariance matrix which are
necessary for a positive-definite matrix in limiting cases
\citep{Messerschmitt:2006}.  We find empirically that requiring an everywhere
positive power spectrum results in positive eigenvalues for the covariance
matrix, and so we describe here how to ensure a positive power spectrum using
Sturm's theorem.
%Relation of positive power spectrum to positive definite covariance matrix.
%Power spectrum of a sum of kernels.

In the case of $J$ \celerite\ terms, we can check for negative values of the
PSD by solving for the roots of the power spectrum, abbreviating with
$z = \omega^2$:
\begin{equation}
P(\omega)=  \sum_{j=1}^J \frac{q_j z + r_j}{z^2+s_jz + t_j} = 0
\end{equation}
where
\begin{eqnarray}
q_j &=& a_jc_j-b_jd_j\\
r_j &=& (d_j^2+c_j^2)(b_jd_j+a_jc_j)\\
s_j &=& 2(c_j^2-d_j^2)\\
t_j &=& (c_j^2+d_j^2)^2.
\end{eqnarray}
The denominators of each term are positive, so we can multiply through by $\Pi_j \left(z^2+s_jz + t_j\right)$ yielding:
\begin{equation}
P_0(z) = \sum_{j=1}^J (q_j z + r_j)\Pi_{k \ne j}\left(z^2+s_kz + t_k\right) = 0,
\end{equation}
which is a polynomial with order $p_{ord}=2(J-1)+1$.  With $J=2$, this yields a cubic equation
which may be solved exactly for the roots.

For arbitrary $J$, a procedure based upon Sturm's theorem \citep{Dorrie:1965} allows one to determine whether there are any
real roots within the range $(0,\infty]$. We first construct $P_0(z)$ and it's derivative
$P_1(z) = P^\prime(z)$, and then loop from $k=2$ to $k=p_{ord}$, computing
$P_k(z) = -{\rm rem}(P_{k-2},P_{k-1})$.  The function ${\rm rem}(p,q)$ is the remainder polynomial after dividing $p(z)$ by $q(z)$.

We evaluate the $z^0$ coefficients of each of the polynomial in the series by evaluating $f_0 = \{P_0(0),...,P_{p_{ord}}(0)\}$ to
give us the signs of these polynomials evaluated at $z=0$.
Likewise, we evaluate the coefficients of the largest order term in each polynomial which gives the sign of the polynomial
as $z \rightarrow \infty$.  This gives $f_\infty = \{C(P_0,p_{ord}),C(P_1,p_{ord}-1),..., C(P_{p_{ord}},1)\}$ where
$C(p(z),m)$ returns the coefficient of $z^m$ in polynomial $p(z)$.

With the series of coefficients $f_0$ and $f_\infty$, we then determine how many times the sign changes in each
of these, where $\sigma(0)$ is the number of sign changes at $z=0$, and $\sigma(\infty)$ is the number of sign
changes at $z \rightarrow \infty$.  The total number of real roots in the range $(0,\infty]$ is given
by $N_{+}=\sigma(0)-\sigma(\infty)$.

We have checked that this procedure works for a wide range of parameters, and we find that it robustly
matches the number of positive real roots which we evaluated numerically.

The advantage of this procedure is that it does not require computing the roots, but only carrying out algebraic
manipulation of polynomials to determine the number of positive real roots.  If a non-zero real root is found, then
likelihood may be set to zero.

We have implemented this algorithm in subroutines that may be called to check for positive-definiteness
of a kernel with some values of $a_j$ or $b_j$ negative.
%Sturm's theorem applied to numerator of power spectrum.


\vspace{1.5em}
All of the code used in this project is available from
\url{https://github.com/dfm/celerite} under the MIT open-source software
license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).

\acknowledgments
It is a pleasure to thank
Sivaram Ambikasaran,
Megan Bedell,
Will Farr,
Sam Grunblatt,
David W.\ Hogg, and
Dan Huber
for helpful contributions to the ideas and code presented here.

EA acknowledges support from NASA grants NNX13AF20G, NNX13AF62G, and
NASA Astrobiology Institute's Virtual Planetary Laboratory, supported
by NASA under cooperative agreement NNH05ZDA001C.

This research made use of the NASA \project{Astrophysics Data System} and the
NASA Exoplanet Archive.
The Exoplanet Archive is operated by the California Institute of Technology,
under contract with NASA under the Exoplanet Exploration Program.

This paper includes data collected by the \kepler\ mission. Funding for the
\kepler\ mission is provided by the NASA Science Mission directorate.
We are grateful to the entire \kepler\ team, past and present.

These data were obtained from the Mikulski Archive for Space Telescopes
(MAST).
STScI is operated by the Association of Universities for Research in
Astronomy, Inc., under NASA contract NAS5-26555.
Support for MAST is provided by the NASA Office of Space Science via grant
NNX13AC07G and by other grants and contracts.

\facility{Kepler}
\software{%
     \project{corner.py} \citep{Foreman-Mackey:2016},
     \project{Eigen} \citep{Guennebaud:2010},
     \project{emcee} \citep{Foreman-Mackey:2013},
     \project{george} \citep{Ambikasaran:2016},
     \project{LAPACK} \citep{Anderson:1999},
     \project{matplotlib} \citep{Hunter:2007},
     \project{numpy} \citep{Van-Der-Walt:2011},
     \project{transit} \citep{Foreman-Mackey:2016a},
     \project{scipy} \citep{Jones:2001}.
}

\bibliography{celerite}

\end{document}
