% Copyright 2015-2016 Dan Foreman-Mackey and the co-authors listed below.

\documentclass[manuscript, letterpaper]{aastex6}

\pdfoutput=1

\include{vc}
\usepackage{microtype}

\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% ----------------------------------- %
% start of AASTeX mods by DWH and DFM %
% ----------------------------------- %

\setlength{\voffset}{0in}
\setlength{\hoffset}{0in}
\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\headheight}{0ex}
\setlength{\headsep}{\baselinestretch\baselineskip} % this is 2 lines in ``manuscript''
\setlength{\footnotesep}{0in}
\setlength{\topmargin}{-\headsep}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}

\linespread{0.54} % close to 10/13 spacing in ``manuscript''
\setlength{\parindent}{0.54\baselineskip}
\hypersetup{colorlinks = false}
\makeatletter % you know you are living your life wrong when you need to do this
\long\def\frontmatter@title@above{
\vspace*{-\headsep}\vspace*{\headheight}
\noindent\footnotesize
{\noindent\footnotesize\textsc{\@journalinfo}}\par
{\noindent\scriptsize Preprint typeset using \LaTeX\ style AASTeX6 with modifications
}\par\vspace*{-\baselineskip}\vspace*{0.625in}
}%
\makeatother

% Section spacing:
\makeatletter
\let\origsection\section
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}
\newcommand\nostarsection[1]{\sectionprelude\origsection{#1}}
\newcommand\starsection[1]{\sectionprelude\origsection*{#1}}
\newcommand\sectionprelude{\vspace{1em}}
\let\origsubsection\subsection
\renewcommand\subsection{\@ifstar{\starsubsection}{\nostarsubsection}}
\newcommand\nostarsubsection[1]{\subsectionprelude\origsubsection{#1}}
\newcommand\starsubsection[1]{\subsectionprelude\origsubsection*{#1}}
\newcommand\subsectionprelude{\vspace{1em}}
\makeatother

\widowpenalty=10000
\clubpenalty=10000

\sloppy\sloppypar

% ------------------ %
% end of AASTeX mods %
% ------------------ %

% Projects:
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\kepler}{\project{Kepler}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\sectionname}{Section}
\newcommand{\sectref}[1]{\ref{sect:#1}}
\newcommand{\Sect}[1]{\sectionname~\sectref{#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\sectref{#1}}
\newcommand{\App}[1]{Appendix~\sectref{#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\agoltodo}[1]{\todo{Agol}{blue}{#1}}


% \shorttitle{}
% \shortauthors{}
% \submitted{Submitted to \textit{The Astrophysical Journal}}

\begin{document}

\title{%
Fast and scalable Gaussian process modeling of stellar variability
\vspace{-3\baselineskip}  % OMG AASTEX6 IS SO BROKEN
}

\newcounter{affilcounter}
% \altaffiltext{1}{}

\setcounter{affilcounter}{1}
\edef \uw {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\uw}       {Astronomy Department, University of Washington,
                          Seattle, WA, 98195, USA}

\edef \sagan {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\sagan}{Sagan Fellow}

\author{%
    Daniel~Foreman-Mackey\altaffilmark{\uw,\sagan} and
    Eric~Agol\altaffilmark{\uw}
}



\begin{abstract}

We derive a fast, $O(N)$, Gaussian Process approach that involves Lorentzian
kernels.  This basis set can approximate commonly used kernels, and is a good
description of stellar variability.

\end{abstract}

\keywords{%
% methods: data analysis
% ---
% methods: statistical
% ---
% catalogs
% ---
% planetary systems
% ---
% stars: statistics
}

\section{Introduction}

\section{Outline}

\begin{enumerate}
\item Non-parametric modeling of time series with correlated noise using Gaussian processes
\item Assumptions:\\
  a). Stationary noise (although this can be broken).\\
  b). Gaussian
\item Problem: can't handle large datasets - e.g. entire Kepler long-cadence; Spitzer light
   curves; Solar curves; short-cadence.
\item Solution(s):\\
  a). HODLR \citep{Ambikasaran2013,Ambikasaran2016}\\
  b). CARMA (Lorentzian power spectra, but with limitations) \citep{Kelly2014}\\
  c). Tri-diagonal (Press \& Rybicki) \citep{1995PhRvL..74.1060R}
\item Problem: HODLR is tricky to use; CARMA requires certain relations between
  coefficients of Lorentzians (right?); P\&R only works for up to 2 components + white noise
\item Solution: Ambikasaran \citep{Ambikasaran2015} shows how sum of exponential kernels can be solved in order
   $O(N*p^2)$, where p is number of Lorentzians.
\item Generalized Ambikasarn for complex coefficients (Hermitian).
\item Performance:\\
  a). Show $O(N*p^2)$ scaling\\
  b). Show comparison to HODLR.\\
  c). Show comparison to wavelet (ala Carter \& Winn).\\
  d). Compare with CARMA Kallman filter(?).
\item Various methods/formulae:\\
  a). Likelihood computation.\\
  b). Generating GP with particular covariance [yet to solve]. [ ]\\
  c). Inferring particular components.\\
  d). Computing derivatives of GPs (as used in RV methods).\\
  e). Forecasting/interpolating.\\
  f). Inferring confidence intervals.\\
  g). Derivative of likelihood function with respect to kernel parameters.\\
  h). Non-stationary GP (can the coefficients vary with time?).\\
  i). Multi-band (each element of extended matrix becomes a matrix).

\item Generalization to non-stationary noise.
\item Example application: TYC 3559\\
  a). Entire long-cadence 17-quarter Kepler dataset (~65k data points).\\
  b). Analysis of solar data.

\item Possible future applications:\\
  a). Inference of asteroseismic variability - ala Andrew Gordon Wilson \citep{2013arXiv1302.4245W}, but
      with Lorentzians, not Gaussians.\\
  b). Multi-waveband GPs.\\
  c). RV interpolation.\\
  d). Non-parametric phase functions.\\
  e). Better calibration of the flicker log(g) relation.\\
  f). Measurement of rotational periods - gyrochronology.\\
  g). Doppler shifts in oscillating EBs.

\item Appendix:
  a). Description/summary of mathematics.\\
  b). Description of code.
\end{enumerate}

\subsection{Derivative of likelihood with respect to Kernel parameters}

The derivative of the likelihood can be computed in $O(N)$ operations as follows.

The derivative of the log likelihood with respect to model parameters ${\bf x}$ is given by:
\begin{equation}
    \frac{\partial \ln{\cal L}}{\partial x_i} = (\ln{\cal L})^\prime = -\frac{1}{2}\sum_i \frac{K_{ii}^\prime}{K_{ii}}
    - \frac{1}{2} {\bf y}^T {\bf K}^{-1} {\bf K}^\prime {\bf K}^{-1} {\bf y},
\end{equation}
where $^\prime$ means the derivative wrt $x_i$.
The kernel may be written as:
\begin{equation}
    K_{ij}({\bf \alpha},{\bf \beta}) = \left\{
    \begin{array}{cc}
       w_i + \sum_{l=1}^p \alpha_l & j = i \\
       \sum_{l=1}^p \alpha_l \exp{(-\beta_l |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.
\end{equation}
Since this equation is the sum of $p$ semi-separable matrices, then applying the product rule to compute the derivative will give a single semi-separable matrix for the derivative with respect to each kernel parameter.

Now, letting ${\bf x} = (w,{\bf \alpha},{\bf \beta})$, then the derivative may be written as:
\begin{eqnarray}
    \frac{\partial K_{ii}}{\partial w} &=& \delta_{ij}\\
    \frac{\partial K_{ij}}{\partial \alpha_k} &=&
     \left\{
    \begin{array}{cc}
       1 & j = i \\
        \exp{(-\beta_k |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.\\
    \frac{\partial K_{ij}}{\partial \beta_k} &=&
     \left\{
    \begin{array}{cc}
       0  & j = i \\
        -\alpha_k|t_i-t_j|\exp{(-\beta_k |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.
\end{eqnarray}
The first term in the derivative of the likelihood can be computed in $O(N)$
evaluations, while the second term can be computed in three steps:
\begin{itemize}
\item First, solve ${\bf z_1} = {\bf K}^{-1}{\bf y}$ with the GRP solver.
\item Next, create an e
\end{itemize}
Writing this term out:

\section{Summary}

\vspace{1.5em}
All of the code used in this project is available from
\url{https://github.com/dfm/GenRP} under the MIT open-source software
license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).


\acknowledgments
It is a pleasure to thank
...
for helpful contributions to the ideas and code presented here.

EA acknowledges support from NASA grants NNX13AF20G, NNX13AF62G, and
NASA Astrobiology Instituteâ€™s Virtual Planetary Laboratory, supported
by NASA under cooperative agreement NNH05ZDA001C.

This research made use of the NASA \project{Astrophysics Data System} and the
NASA Exoplanet Archive.
The Exoplanet Archive is operated by the California Institute of Technology,
under contract with NASA under the Exoplanet Exploration Program.

This paper includes data collected by the \kepler\ mission. Funding for the
\kepler\ mission is provided by the NASA Science Mission directorate.
We are grateful to the entire \kepler\ team, past and present.

These data were obtained from the Mikulski Archive for Space Telescopes
(MAST).
STScI is operated by the Association of Universities for Research in
Astronomy, Inc., under NASA contract NAS5-26555.
Support for MAST is provided by the NASA Office of Space Science via grant
NNX13AC07G and by other grants and contracts.

\facility{Kepler}
\software{%
    % \project{ceres} \citep{Agarwal:2016},
    % \project{corner.py} \citep{Foreman-Mackey:2016},
    % \project{emcee} \citep{Foreman-Mackey:2013},
    % \project{george} \citep{Ambikasaran2016},
	% \project{matplotlib} \citep{Hunter:2007},
	% \project{numpy} \citep{Van-Der-Walt:2011},
	% \project{scipy} \citep{Jones:2001}.
}

\appendix

Here we adapt the generalized Rybicki-Press (GRP) algorithm to handle periodic kernels.  We
follow closely the notation and equations given in \citet{Ambikasaran2015}.  We start
with the observation that a Lorentzian correlation function may be decomposed into the sum of two
exponential functions:
\begin{eqnarray}
K(\tau) &=& \alpha e^{-\beta_R \tau} \cos{(\omega \tau)}\\
 &=& \frac{1}{2}\alpha \left[e^{-\beta \tau}+e^{-\beta^* \tau}\right],
\end{eqnarray}
where $\beta = \beta_R + i\beta_I$ is the 
complex kernel parameter (i.e.\ $\beta_R = Re(\beta)$, $\beta_I = Im(\beta)$), $\omega = -\beta_I = 2\pi/P$
is the frequency of oscillation of the kernel with period $P$, and $x^*$ is the complex conjugate of $x$.

Now, the Lorentzian kernal may be included in the GRP algorithm 
using the two exponential kernels with $\beta_1 = \beta$ and $\beta_2 = \beta^*$.  This 
requires the algorithm to use complex arithmetic, which has several disadvantages:
1).\ complex arithmetic is more expensive by a factor of 2-3; 2).\ the complex conjugate equation
is redundant as it turns out that $l_2 = l_1^*$, $r_2 = r_1^*$, and $x_i = x_i^*$ (i.e.\ $x_i$ is
real), meaning that the same equations are being solved twice for the real and complex components; 
and 3).\ if the derivative of the likelihood is computed with automatic
differentiation (AD), most AD algorithms require real variables.

Given these drawbacks, we instead modify the GRP algorithm to use a combination of two real
components rather than two complex components;  this addresses all three of these drawbacks
simultaneously.  The one cost is that the banded extended matrix has a bandwidth which is
larger by one, both above and below the diagonal.

Rather than using $\beta$ and its complex conjugate as components of the kernel, we instead
find the real and imaginary components of the complex equationi for $\beta$, and 
discard the $\beta^*$ equation, which is redundant.  This gives four additional equations for
each $\beta_j$ with non-zero imaginary component, while the kernel components with real
$\beta_j$ still only have two additional components in the extended matrix.  We will enumerate
the number of real $\beta$'s as $p_0$, while the total number of $\beta$'s is still $p$.
This gives a total number of equations to be solved with the extended matrix of $N_{ex}= 
(4(p-p_0)+2p_0+1)\times(N-1)+1$, where $n$ is the number of data points.  

For the complex values of $\beta$, the equations are modified as follows.  We introduce complex
auxiliary vectors $l_k = l_{k}^R - i l_{k}^I$, with $k \in \{1,2,...,N\}$ and $l_1 = 0$; $r_k =
r_k^R - i r_k^I$, with $k \in \{2,...,N\}$ and $r_{N+1} = 0$.  Note that we have defined the
imaginary component as being negative for aesthetic reasons:  this yields a symmetric extended
matrix (unfortunately the extended matrix has negative eigenvalues, making it non positive-definite,
so that Cholesky decomposition cannot be applied; nevertheless we prefer to use a symmetric 
set of equations, and since the values of these variables are discarded in the end, their sign
is irrelevant).  The length of the vectors $l_k$ and $r_k$
is the number of kernel components, $p$;  we denote the individual elements of these vectors
as $l_{k,j}$ and $r_{k,j}$, $j \in \{1,...,p\}$.  Note that the first $p_0$ elements we take
to be the real values of $\beta$, and for these we do not include the imaginary component
equations, which are unnecessary and only add extra computation time.  We allow each vector $\gamma_k$ to have real and imaginary
components as well:  $\gamma_k = \left[\exp{(-\beta_1 t_{k,k+1})} ... \exp{(-\beta_p t_{k,k+1})}\right]^T
 = \gamma_k^R + i \gamma_k^I.$  These elements are given by $\gamma_{k,j}^R = e^{-\beta_j t_{k,k+1}}
\cos{(\beta_j^I t_{k,k+1})}$ and $\gamma_{k,j}^I = -e^{-\beta_j t_{k,k+1}}\sin{(\beta_j^I t_{k,k+1})}$.
Equation (61) becomes:
\begin{equation}
% See 9/2/2016 notes
\sum_{j=1}^p \frac{\alpha_i}{2} l_{k,j}^R + \frac{d}{2} x_k + \sum_{j=1}^p \left[
\gamma_{k,j}^R r_{k+1,j}^R + \gamma_{k,j}^I r_{k+1,j}^I\right] = \frac{b_k}{2},
\end{equation}
for $j \in \{1,...,p\}$ and $k \in \{1,...,N\}$.  There is no imaginary component to equation (61).

Equation (60) has real and imaginary components:
\begin{eqnarray}
\gamma_{k,j}^R l_{k,j}^R +\gamma_{k,j}^I l_{k,j}^I + \gamma_{k,j}^R x_k - l_{k+1,j}^R &=& 0,\\
\gamma_{k,j}^I l_{k,j}^R -\gamma_{k,j}^R l_{k,j}^I + \gamma_{k,j}^I x_k + l_{k+1,j}^I &=& 0,
\end{eqnarray}
with $j \in \{1,...,p\}$ and $k \in \{1,...,N-1\}$
\citep[note that we have shifted the indices of this equation by one from the indexing using in][]{Ambikasaran2015}.

Finally, equation (59) has real and imaginary components:
\begin{align}
-&r_{k,j}^R + \frac{\alpha_j}{2} x_k &+ \gamma_{k,j}^R r_{k+1,j}^R + \gamma_{k,j}^I r_{k+1,j}^I &&=0,\\
 &r_{k,j}^I &+ \gamma_{k,j}^I r_{k+1,j}^R - \gamma_{k,j}^R r_{k+1,j}^I &&=0,
\end{align}
with $j \in \{1,...,p\}$ and $k \in \{2,...,N\}$.  Note that for the $p_0$ real components
we can drop the variables $r_{k,j}^I$ and $l_{k,j}^I$ as these are zero, and we can drop the
imaginary components of equations (59) and (60) as well for these, which reduces the total number
of equations to solve.

With these additional equations, we can solve for the likelihood of sums of full Lorentzian kernels 
in $O(N)$ operations, but with slightly more computational expense due to having 4 extra variables 
for each Lorentzian with non-zero $\omega$, and due to having a slightly larger bandwidth due to the 
cross terms between the real and imaginary variables.  The off-diagonal components are $2(p+1)$ above 
and below the diagonal, for a total bandwidth of $4p+5$; in the case in which $p_0=p$ so that the
imaginary components of $\beta$ are zero, the off-diagonal components revert to $p+1$, giving
a bandwidth of $2p+3$.

As with the GRP method, we can buid an extended matrix which is real and symmetric, and which may be 
solved with a banded solver, as in \citet{NumericalRecipes}.  The determinant of this extended matrix 
may be computed from the LU decomposition, and it is related to the determinant of the kernel by a 
factor of $2^N$ (this is due to dividing each equation by $2$ to yield a symmetric matrix).
We arrange the auxiliary variables with real values of 
$\beta$ first, followed by the complex variables, yielding the vector
$x_{ex} = [x_1, \{r_{2,j}^R,l_{2,j}^R;j=1,...,p_0\}, \{r_{2,j}^R,r_{2,j}^I,l_{2,j}^R,l_{2,j}^I;j=p_0+1,...,p\},
x_2, ..., x_N]$.

The matrix has a similar structure as the real case constructed by \citep{Ambikasaran2015}, but the
imaginary and real components mix (this is due to the representation of imaginary numbers as
$2\times 2$ matrices).  Figure \ref{matrix_structure} shows an example of the structure of the extended
matrix for two Lorentzians with non-zero $\omega$ values.  This is analagous to the real case with
$p=2$ which also has four additional equations for each row of the original kernel.  This matrix shows
that there is an extra band above and below the diagonal \citep[compare with Figure 2 of][]{Ambikasaran2015}.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.175]{./twoterm.eps}
\hspace{0.2in}
\includegraphics[scale=1.0]{./images/colorcode.eps}
\end{center}
\caption{Pictorial description of the extended sparse matrix where $N=10$, $p_0=0$, and $p=2$, following
the example shown in \citet{Ambikasaran2015}.}
\label{matrix_structure}
\end{figure}

{\bf Should I give some examples of this?  Some pseudocode?}

\bibliography{genrp}

\end{document}
