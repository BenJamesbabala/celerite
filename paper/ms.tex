% Copyright 2015-2017 Dan Foreman-Mackey and the co-authors listed below.

\documentclass[manuscript, letterpaper]{aastex6}

\pdfoutput=1

\include{vc}
\include{figures/rotation}

\usepackage{microtype}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% ----------------------------------- %
% start of AASTeX mods by DWH and DFM %
% ----------------------------------- %

% Matrix fix:
% http://tex.stackexchange.com/questions/317824/letter-c-appearing-inside-pmatrix-environment-with-aastex
\makeatletter
\def\env@matrix{\hskip -\arraycolsep % taken from amsmath.sty lines 895ff
  \let\@ifnextchar\new@ifnextchar
  \array{*{\c@MaxMatrixCols}c}}
\makeatother

% Column spacing in matrix
% http://tex.stackexchange.com/questions/275725/adjusting-separation-between-matrix-entries
\setlength\arraycolsep{25pt}

\setlength{\voffset}{0in}
\setlength{\hoffset}{0in}
\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\headheight}{0ex}
\setlength{\headsep}{\baselinestretch\baselineskip} % this is 2 lines in ``manuscript''
\setlength{\footnotesep}{0in}
\setlength{\topmargin}{-\headsep}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}

\linespread{0.54} % close to 10/13 spacing in ``manuscript''
\setlength{\parindent}{0.54\baselineskip}
\hypersetup{colorlinks = false}
\makeatletter % you know you are living your life wrong when you need to do this
\long\def\frontmatter@title@above{
\vspace*{-\headsep}\vspace*{\headheight}
\noindent\footnotesize
{\noindent\footnotesize\textsc{\@journalinfo}}\par
{\noindent\scriptsize Preprint typeset using \LaTeX\ style AASTeX6 with modifications
}\par\vspace*{-\baselineskip}\vspace*{0.625in}
}%
\makeatother

% Section spacing:
\makeatletter
\let\origsection\section
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}
\newcommand\nostarsection[1]{\sectionprelude\origsection{#1}}
\newcommand\starsection[1]{\sectionprelude\origsection*{#1}}
\newcommand\sectionprelude{\vspace{1em}}
\let\origsubsection\subsection
\renewcommand\subsection{\@ifstar{\starsubsection}{\nostarsubsection}}
\newcommand\nostarsubsection[1]{\subsectionprelude\origsubsection{#1}}
\newcommand\starsubsection[1]{\subsectionprelude\origsubsection*{#1}}
\newcommand\subsectionprelude{\vspace{1em}}
\makeatother

\widowpenalty=10000
\clubpenalty=10000

\sloppy\sloppypar

% ------------------ %
% end of AASTeX mods %
% ------------------ %

% Projects:
\newcommand{\project}[1]{\textsf{#1}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\tess}{\project{TESS}}
\newcommand{\celerite}{\project{celerite}}
\newcommand{\emcee}{\project{emcee}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\sectionname}{Section}
\newcommand{\sectref}[1]{\ref{sect:#1}}
\newcommand{\Sect}[1]{\sectionname~\sectref{#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\sectref{#1}}
\newcommand{\App}[1]{Appendix~\sectref{#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\agoltodo}[1]{\todo{Agol}{blue}{#1}}


% \shorttitle{}
% \shortauthors{}
% \submitted{Submitted to \textit{The Astrophysical Journal}}

\begin{document}

\title{%
Fast and scalable Gaussian process modeling with applications
to astronomical time series
\vspace{-3\baselineskip}
}

\newcounter{affilcounter}
% \altaffiltext{1}{}

\setcounter{affilcounter}{1}

\edef \sagan {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\sagan}{Sagan Fellow}

\edef \uw {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\uw}{Astronomy Department, University of Washington,
                   Seattle, WA}

\edef \simons {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\simons}{Simons Fellow}

\edef \columbia {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\columbia}{Department of Astronomy, Columbia University,
                         New York, NY}

\edef \iss {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\iss}{Department of Computational and Data Sciences,
                    Indian Institute of Science, Bangalore, India}


\author{%
    Daniel~Foreman-Mackey\altaffilmark{\sagan,\uw},
    Eric~Agol\altaffilmark{\uw},
    Ruth~Angus\altaffilmark{\simons,\columbia}, and
    Sivaram~Ambikasaran\altaffilmark{\iss}
}



\begin{abstract}

\dfmtodo{Make this less douchey:}
In the current era of large-scale time domain astronomy, scalable methods for
probabilistic inference are of great interest.
We present a method for Gaussian Process regression in one dimension with a
computational cost that scales linearly with the size of the dataset.
This method exploits structure in the problem when the covariance function is
expressed as a mixture of complex exponentials but it does not require evenly
spaced observations or homoscedastic noise.
This form of the covariance arises naturally when the process is a mixture of
stochastically-driven damped harmonic oscillators~--~providing a physical
motivation for and interpretation of this choice~--~but we also demonstrate
that this function can also be a sufficient effective model in many other
cases.
We present a mathematical description of the method, details of the
implementation, and compare it to existing scalable Gaussian Process methods.
We demonstrate the application of this method by applying it to simulated and
real astronomical time series datasets.
These demonstrations are examples of probabilistic inference of stellar
rotation periods, asteroseismic oscillation spectra, and transiting planet
parameters.
The method is flexible, fast, and most importantly, interpretable.
There are a wide range of potential applications within astronomical data
analysis and beyond.
We provide well tested and documented open-source implementations of this
method in \project{C++}, \project{Python}, and \project{Julia}.

\end{abstract}

\keywords{%
% methods: data analysis
% ---
% methods: statistical
% ---
% catalogs
% ---
% planetary systems
% ---
% stars: statistics
}

\section{Introduction}

Gaussian Processes \citep[GPs;][]{Rasmussen:2006} are popular stochastic
models for time-series analysis.
For GP modeling, a functional form is chosen to describe the autocovariance
of the data and the parameters of this function are fit for or marginalized.
In the astrophysical literature, GPs have been used to model stochastic
variability in light curves of stars \citep{Brewer:2009}, active galactic
nuclei \citep{Kelly:2014}, and the logarithmic flux of X-ray binaries
\citep{Uttley:2005}.
They have also been used as models for the cosmic microwave background
\citep{Bond:1987,Bond:1999}, correlated instrumental noise
\citep{Gibson:2012}, spectroscopic calibration \citep{Czekala:2017,Evans:2015}
and residuals caused by model inconsistencies (CITE + better words).
While these models are widely applicable, their use has been limited, in
practice, by the computational cost and scaling.
In general, the cost of computing a GP likelihood scales as the third power of
the number of data points $\mathcal{O}(N^3)$ and in the current era of large
time domain surveys~--~with as many as $\sim10^{4-9}$ targets with
$\sim10^{3-5}$ observations each~---~this cost is prohibitive.

In this paper, we present a method for computing a class of GP models that
scales linearly with the number of data points $\mathcal{O}(N)$ for one
dimensional data sets.
This method is a generalization of a method developed by
\citet{Ambikasaran:2015} that was, in turn, built on intuition from a twenty
year old paper \citep{Rybicki:1995}.
For this method to be applicable, the data must be one-dimensional and the
covariance function must have a specific form.
However, there is no further constraint on the data or the model.
In particular, the measurements don't need to be evenly spaced and the
uncertainties can be heteroscedastic.
This method is especially appealing compared to other similar methods~--~we
will return to these below~--~because it is exact, flexible, simple, and fast.

In the following pages, we will motivate the general problem of GP regression,
describe the previously published scalable method \citep{Rybicki:1995,
Ambikasaran:2015} and our generalization, and demonstrate the model's
application on various real and simulated data sets.
Alongside this paper, we have released well tested and documented open source
implementations written in \project{C++}, \project{Python}, and
\project{Julia}.
These are available online at \project{GitHub}
\url{https://github.com/dfm/celerite} and \project{Zenodo} \dfmtodo{add zenodo
archive}.

\section{Gaussian processes}\sectlabel{gps}

Gaussian Processes \citep[GPs;][]{Rasmussen:2006} are a class of stochastic
models consisting of a mean function $\mu_\bvec{\theta}(\bvec{x})$ and a
covariance, autocorrelation, or ``kernel'' function
$k_\bvec{\alpha}(\bvec{x}_i,\,\bvec{x}_j)$ parameterized by the parameters
$\bvec{\theta}$ and $\bvec{\alpha}$ respectively.
Under this model, the log-likelihood of observing a dataset
\begin{eqnarray}
\bvec{y} = \left(\begin{array}{ccccc}
    y_1\quad && \cdots\quad && y_N
\end{array}\right)^\T
\end{eqnarray}
at coordinates
\begin{eqnarray}
X = \left(\begin{array}{ccccc}
    \bvec{x}_1\quad && \cdots\quad && \bvec{x}_N
\end{array}\right)^\T
\end{eqnarray}
is
\begin{eqnarray}\eqlabel{gp-likelihood}
\ln{p(\bvec{y}\,|\,{X,\,\bvec{\theta}},\,\bvec{\alpha})} =
    -\frac{1}{2} {\bvec{r}_\bvec{\theta}}^\T\,{K_\bvec{\alpha}}^{-1}\,
        \bvec{r}_\bvec{\theta}
    -\frac{1}{2}\ln\det K_\bvec{\alpha}
    - \frac{N}{2} \ln{(2\pi)}
\end{eqnarray}
where
\begin{eqnarray}
    \bvec{r}_\bvec{\theta} = \left(\begin{array}{ccccc}
    y_1 - \mu_\bvec{\theta}(\bvec{x}_1)\quad && \cdots\quad &&
    y_N - \mu_\bvec{\theta}(\bvec{x}_N)
\end{array}\right)^\T
\end{eqnarray}
is the vector of residuals and the elements of the covariance matrix $K$ are
given by $[K_\bvec{\alpha}]_{nm} = k_\bvec{\alpha}(\bvec{x}_n,\,\bvec{x}_m)$.
The maximum likelihood values for the parameters $\bvec{\theta}$ and
$\bvec{\alpha}$ for a given dataset $(\bvec{y},\,X)$ can be found by
maximizing \eq{gp-likelihood} with respect to $\bvec{\theta}$ and
$\bvec{\alpha}$ using a non-linear optimization routine \citep{Nocedal:2006}.
Similarly, probabilistic constraints on $\bvec{\theta}$ and $\bvec{\alpha}$
can be obtained by multiplying the likelihood by a prior
$p(\bvec{\theta},\,\bvec{\alpha})$ and using a Markov Chain Monte Carlo (MCMC)
algorithm to sample from the posterior probability density.

GP models have been widely used across the physical sciences but their
application is generally limited to small datasets because the computational
cost of computing the inverse and determinant of the matrix $K_\bvec{\alpha}$
is $\mathcal{O}(N^3)$.
In other words, this cost is proportional to the cube of the number of data
points $N$.
This means that for large datasets, every evaluation of the likelihood will
quickly become computationally intractable.
In this case, standard non-linear optimization or MCMC will no longer be
practical inference methods.

In the following Section, we present a method of substantially improving this
scaling in many circumstances.
We call our method and its implementations \celerite.\footnote{The name
\celerite\ comes from the French word \foreign{c\'elerit\'e} meaning the speed
of light in a vacuum.} The \celerite\ method requires using a specific model
for the covariance $k_\bvec{\alpha}(\bvec{x}_n,\,\bvec{x}_m)$ and it has
several limitations but, in subsequent sections, we demonstrate that it can be
used to increase the computational efficiency of many astronomical data
analysis problems.
The main limitation of this method is that it can only be applied to
one-dimensional datasets.
When we say ``one-dimensional'' here, it means that the \emph{input
coordinates} $\bvec{x}_n$ are scalar, $\bvec{x}_n \equiv t_n$.\footnote{We are
using $t$ as the input coordinate because one-dimensional GPs are often
applied to time series data but this isn't a real restriction and the \celerite\
method can be applied to \emph{any} one-dimensional dataset.}
Furthermore, the covariance function for the \celerite\ method is ``stationary''.
This means that the function $k_\bvec{\alpha}(t_n,\,t_m)$ is only a function
of $\tau_{nm} \equiv |t_n - t_m|$.


\section{The celerite model}

To scale GP models to larger datasets, \citet{Rybicki:1995} presented a method
of computing the first term in \eq{gp-likelihood} in $\mathcal{O}(N)$
operations when the covariance function is given by
\begin{eqnarray}\eqlabel{kernel-simple}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} + a\,\exp(-c\,\tau_{nm})
\end{eqnarray}
where $\{{\sigma_n}^2\}_{n=1}^N$ are the measurement uncertainties,
$\delta_{nm}$ is the Kronecker delta, and $\bvec{\alpha} = (a,\,c)$.
The intuition behind this method is that, for this choice of $k_\bvec{\alpha}$,
the inverse of $K_\bvec{\alpha}$ is tridiagonal and can it can be computed
with a small number of operations for each data point.
Subsequently, \citet{Ambikasaran:2015} generalized this method to arbitrary
mixtures of exponentials
\begin{eqnarray}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J a_j\,\exp(-c_j\,\tau_{nm})\quad.
\end{eqnarray}
In this case, the inverse will be dense but \eq{gp-likelihood} can still be
evaluated in $\mathcal{O}(N)$ operations where $J$ is the number of components
in the mixture and $N$ is still the number of data points.

It turns out that this kernel function can be made even more general by
introducing complex parameters $a_j \to a_j\pm i\,b_j$ and
$c_j \to c_j\pm i\,d_j$.
In this case, the covariance function becomes
\begin{eqnarray}\eqlabel{celerite-kernel-complex}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J &&\left[
    \frac{1}{2}(a_j + i\,b_j)\,\exp\left(-(c_j+i\,d_j)\,\tau_{nm}\right)
        \right. \nonumber\\
    &&+\left.
    \frac{1}{2}(a_j - i\,b_j)\,\exp\left(-(c_j-i\,d_j)\,\tau_{nm}\right)
\right]
\end{eqnarray}
and, for this function, \eq{gp-likelihood} can still be solved with
$\mathcal{O}(N)$ operations.
The details of this method and a few implementation considerations are
discussed in the following \sectionname\ but we will first discuss some
properties of this covariance function.

By rewriting the exponentials in \eq{celerite-kernel-complex} as sums of sine
and cosine functions, we can see the autocorrelation structure is defined by a
mixture of quasiperiodic oscillators
\begin{eqnarray}\eqlabel{celerite-kernel}
k_\bvec{\alpha}(\tau_{nm}) = \sigma_n^2\,\delta_{nm} +
    \sum_{j=1}^J &&\left[
    a_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\cos\left(d_j\,\tau_{nm}\right)
        \right.\nonumber\\
    &&+ \left.
    b_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\sin\left(d_j\,\tau_{nm}\right)
\right] \quad.
\end{eqnarray}
For clarity, we will refer to the argument within the sum as a ``\celerite\
term'' for the remainder of this paper.
The Fourier transform\footnote{Here and throughout we have defined the Fourier
transform of the function $f(t)$ as $F(\omega)={(2\,\pi)}^{-1/2}\,
\int_{-\infty}^\infty f(t)\,e^{i\,\omega\,t}\dd t$.} of this covariance
function is the power spectral density (PSD) of the process and it is given by
\begin{eqnarray}\eqlabel{celerite-psd}
S(\omega) = \sum_{j=1}^J \sqrt{\frac{2}{\pi}}
\frac{(a_j\,c_j+b_j\,d_j)\,({c_j}^2+{d_j}^2)+(a_j\,c_j-b_j\,d_j)\,\omega^2}
{\omega^4+2\,({c_j}^2-{d_j}^2)\,\omega^2+{({c_j}^2+{d_j}^2)}^2}\quad.
\end{eqnarray}
The physical interpretation of this model isn't immediately obvious and we
will return to a more general discussion of the physical intuition in a moment
but we can start with a discussion of some useful special cases of this model.

If we set the imaginary amplitude $b_j$ for some component $j$ to zero, that
term of \eq{celerite-kernel} becomes
\begin{eqnarray}
k_j(\tau_{nm}) =
    a_j\,\exp\left(-c_j\,\tau_{nm}\right)\,\cos\left(d_j\,\tau_{nm}\right)
\end{eqnarray}
and the PSD for the this component is
\begin{eqnarray}\eqlabel{lorentz-psd}
S_j(\omega) = \frac{1}{\sqrt{2\,\pi}}\,\frac{a_j}{c_j}\,\left[
    \frac{1}{1+{\left(\frac{\omega-d_j}{c_j}\right)}^2} +
    \frac{1}{1+{\left(\frac{\omega+d_j}{c_j}\right)}^2}
\right] \quad.
\end{eqnarray}
This is the sum of two Lorentzian or Cauchy distributions with width $c_j$
centered on $\omega = \pm d_j$.
This model can be interpreted intuitively as a quasiperiodic oscillator with
amplitude $A_j = a_j$, quality factor $Q_j = d_j\,{(2\,c_j)}^{-1}$, and period
$P_j = 2\,\pi\,{d_j}^{-1}$.

Similarly, setting both $b_j$ and $d_j$ to zero, we get a Ornstein--Uhlenbeck
process
\begin{eqnarray}
k_j(\tau_{nm}) = a_j\,\exp\left(-c_j\,\tau_{nm}\right)
\end{eqnarray}
with the PSD
\begin{eqnarray}
S_j(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{a_j}{c_j}\,
    \frac{1}{1+{\left(\frac{\omega}{c_j}\right)}^2} \quad.
\end{eqnarray}

Finally, we note that the product of two terms of the form found inside the
sum in \eq{celerite-kernel} can also be re-written as a sum with updated
parameters
\begin{eqnarray}\eqlabel{product-rule}
k_j(\tau) \, k_k(\tau) =
    e^{-\tilde{c}\,\tau}\,[
        a_+\,\cos(d_+\,\tau) + b_+\,\sin(d_+\,\tau) +
        a_-\,\cos(d_-\,\tau) + b_-\,\sin(d_-\,\tau)
    ]
\end{eqnarray}
where
\begin{eqnarray}
    \tilde{a}_{\pm} &=& \frac{1}{2}\,(a_j\,a_k \pm b_j\,b_k) \\
    \tilde{b}_{\pm} &=& \frac{1}{2}\,(b_j\,a_k \mp a_j\,b_k) \\
    \tilde{c} &=& c_j + c_k \\
    \tilde{d}_{\pm} &=& d_j \mp d_k \quad.
\end{eqnarray}
Therefore, the method described in the following section can be used to scale
inferences to large datasets for any model where the kernel function is a
general sum or product of \celerite\ terms.

\section{Implementation \& performance}

\citet{Rybicki:1995} demonstrated that the inverse of a matrix $K$ where the
elements are given by equation \eq{kernel-simple} could be computed
efficiently by taking advantage of the structure of this covariance function
and \citet{Ambikasaran:2015} generalized this computation to apply to the full
mixture of $J$ terms in \eq{celerite-kernel-complex} and derived an equally
efficient method for computing the determinant of $K$.

\subsection{An example}

To provide some insight for this method, we will follow
\citet{Ambikasaran:2015} and start by working through a simple example.
In this case, we'll assume that we have three data points
$\{y_1,\,y_2,\,y_3\}$ observed at times $\{t_1,\,t_2,\,t_3\}$ with measurement
variances $\{{\sigma_1}^2,\,{\sigma_2}^2,\,{\sigma_3}^2\}$ and we would
like to compute the likelihood of these data under a GP model with the
covariance function
\begin{eqnarray}
k(\tau_{nm}) = \sigma_n^2\,\delta_{nm} + a\,\exp(-c\,\tau_{nm})\quad.
\end{eqnarray}
This is the result of setting $J=1$, $b=0$, and $d=0$ in
\eq{celerite-kernel-complex} and this is the model that was studied by
\citet{Rybicki:1995}.
To demonstrate this method, we will write out the full system of
equations that we must solve to apply the inverse of $K$ in order to compute
the first term of \eq{gp-likelihood}.
In matrix notation, this can be written as
\begin{eqnarray}\eqlabel{impl-matrix}
K\,\bvec{z} &=& \bvec{y}, \\
\begin{pmatrix}
    a+{\sigma_1}^2 & a\,e^{-c\,\tau_{2,1}} & a\,e^{-c\,\tau_{3,1}}\\
    a\,e^{-c\,\tau_{2,1}} & a+{\sigma_2}^2 & a\,e^{-c\,\tau_{3,2}}\\
    a\,e^{-c\,\tau_{3,1}} & a\,e^{-c\,\tau_{3,2}} & a+{\sigma_3}^2
\end{pmatrix}\,
\begin{pmatrix}
    z_1 \\ z_2 \\ z_3
\end{pmatrix} &=&
\begin{pmatrix}
    y_1 \\ y_2 \\ y_3
\end{pmatrix}
\end{eqnarray}
where our goal is to solve for the unknown vector \bvec{z} for a given matrix
$K$ and vector \bvec{y}.
In this equation, we have assumed that the mean function is zero but a
non-zero mean could be included by replacing \bvec{y} by
$\bvec{r}_\bvec{\theta}$ as defined in \sect{gps}.
Now, if we introduce the variables
\begin{eqnarray}\eqlabel{algo-first}
    u_n = e^{-c\,\tau_{n+2,n+1}}\,u_{n+1} + a\,z_{n+1}
\end{eqnarray}
where $u_{N} = 0$, and
\begin{eqnarray}
    g_n = e^{-c\,\tau_{n+1,n}}\,g_{n-1} + e^{-c\,\tau_{n+1,n}}\,z_{n}
\end{eqnarray}
where $g_{0} = 0$,
the system of equations can be rewritten as
\begin{eqnarray}
(a+{\sigma_1}^2)\,z_1 + e^{-c\,\tau_{2,1}}\,u_1 &=& y_1 \\
a\,g_1 + (a+{\sigma_2}^2)\,z_2 + e^{-c\,\tau_{3,2}}\,u_2 &=& y_2 \\
a\,g_2 + (a+{\sigma_3}^2)\,z_3 &=& y_3 \quad. \eqlabel{algo-last}
\end{eqnarray}
The system defined by \eq{algo-first} through \eq{algo-last} can be
rewritten as a matrix equation to show the benefit that this seemingly trivial
reformulation provides:
\begin{eqnarray}
\begin{pmatrix}
    a+{\sigma_1}^2 & e^{-c\,\tau_{2,1}} & 0 & 0 & 0 & 0 & 0 \\
    e^{-c\,\tau_{2,1}} & 0 & -1 & 0 & 0 & 0 & 0 \\
    0 & -1 & 0 & a & e^{-c\,\tau_{3,2}} & 0 & 0 \\
    0 & 0 & a & a+{\sigma_2}^2 & e^{-c\,\tau_{3,2}} & 0 & 0 \\
    0 & 0 & e^{-c\,\tau_{3,2}} & e^{-c\,\tau_{3,2}} & 0 & -1 & 0 \\
    0 & 0 & 0 & 0 & -1 & 0 & a \\
    0 & 0 & 0 & 0 & 0 & a & a+{\sigma_3}^2 \\
\end{pmatrix}\,
\begin{pmatrix}
    z_1 \\ u_1 \\ g_1 \\ z_2 \\ u_2 \\ g_2 \\ z_3
\end{pmatrix} &=&
\begin{pmatrix}
    y_1 \\ 0 \\ 0 \\ y_2 \\ 0 \\ 0 \\ y_3
\end{pmatrix}\nonumber
\end{eqnarray}
where we will follow \citet{Ambikasaran:2015} and call this the ``extended''
system and rewrite \eq{impl-matrix} as
\begin{eqnarray}
    K_\mathrm{ext}\,\bvec{z}_\mathrm{ext} &=& \bvec{y}_\mathrm{ext} \quad.
\end{eqnarray}
Even though $K_\mathrm{ext}$ is a larger matrix than the $K$ that we started
with, it is now sparse with banded structure that can be exploited to solve
the system efficiently.
In particular, sparse solvers are available that can perform a
LU-decomposition of matrices like this in $\mathcal{O}(N)$
operations~--~instead of the $\mathcal{O}(N^3)$ that would be required in
general~--~and we can take advantage of these algorithms to solve our system
exactly because the target vector $\bvec{z}$ is a subset of the elements of
$\bvec{z}_\mathrm{ext}$.

In the following section, we will discuss this method more generally but it's
worth noting a few important facts that can already be seen in this example.
First, the fundamental reason why this matrix $K$ can be solved efficiently is
the following property of exponentials
\begin{eqnarray}
    e^{-c\,(t_3 - t_2)} \, e^{-c\,(t_2 - t_1)} =
    e^{-c\,(t_3 - t_2 + t_2 - t_1)} =
    e^{-c\,(t_3 - t_1)}
\end{eqnarray}
and it is important to note that this property does not extend to other common
covariance functions like the ``exponential-squared'' function
\begin{eqnarray}
    k(\tau) \propto e^{-c\,\tau^2} \quad.
\end{eqnarray}
Furthermore, our derivation of the extended matrix requires that the data
points be monotonically sorted in time.
Neither of these properties will be satisfied in general for multidimensional
inputs and all of our following discussion will assume a sorted
one-dimensional dataset.

\citet{Ambikasaran:2015} demonstrated two key facts that allow us to use this
extended matrix formalism in practice.
First, even if the covariance function is a mixture of exponentials, the
extended matrix will still be banded with a bandwidth that scales linearly
with the number of components $J$.
Second, \citet{Ambikasaran:2015} proved that the determinant of
$K_\mathrm{ext}$ is equal to the determinant of $K$ up to a sign.
This means that we can use this extended matrix formalism to compute the
marginalized likelihood in $\mathcal{O}(N)$ operations.

\subsection{The algorithm}

In this section, we generalize the method from the previous section to the
covariance function given by \eq{celerite-kernel}.
This derivation follows \citet{Ambikasaran:2015} but it includes explicit
treatment of complex parameters, and their complex conjugates.

In the case of the full \celerite\ covariance function
(\eqalt{celerite-kernel}), we introduce the following auxiliary variables in
analogy to the $u_n$ and $g_n$ that we introduced in the previous section
\begin{eqnarray}\eqlabel{full-system-first}
\phi_{n,j} &=& e^{-c_j\,\tau_{n+1,n}}\,\cos\left(d_j\,\tau_{n+1,n}\right)\\
\psi_{n,j} &=& -e^{-c_j\,\tau_{n+1,n}}\,\sin\left(d_j\,\tau_{n+1,n}\right)\\
g_{n,j} &=& \phi_{n,j}\,g_{n-1,j} + \phi_{n,j}\,z_n + \psi_{n,j}\,h_{n-1,j}\\
h_{n,j} &=& \phi_{n,j}\,h_{n-1,j} - \psi_{n,j}\,z_n - \psi_{n,j}\,g_{n-1,j}\\
u_{n,j} &=& \phi_{n+1,j}\,u_{n+1,j} + a_j\,z_{n+1} + \psi_{n+1,j}\,v_{n+1,j}\\
v_{n,j} &=& \phi_{n+1,j}\,v_{n+1,j} - b_j\,z_{n+1} - \psi_{n+1,j}\,u_{n+1,j}
\end{eqnarray}
with the boundary conditions
\begin{eqnarray}
    g_{0,j} = 0 \quad, \quad
    h_{0,j} = 0 \quad, \quad
    u_{N,j} = 0 \quad, \quad\mathrm{and}\quad
    v_{N,j} = 0
\end{eqnarray}
for all $j$.
Using these variables and some algebra, we find that the following expression
\begin{eqnarray}\eqlabel{full-system-last}
\sum_{j=1}^J \left[a_j\,g_{n,j}+b_j\,h_{n,j}\right]
+ \left[{\sigma_n}^2+\sum_{j=1}^J a_j\right]
+ \sum_{j=1}^J \left[\phi_{n,j}\,u_{n,j}+\psi_{n,j}\,v_{n,j}\right]
    &=& r_{\bvec{\theta},n}
\end{eqnarray}
is equivalent to the target matrix equation
\begin{eqnarray}
K\,\bvec{z} &=& \bvec{r}_\bvec{\theta}
\end{eqnarray}
if $r_{\bvec{\theta},n}$ is the $n$-th element of the residual vector
$\bvec{r}_\bvec{\theta}$ defined in \sect{gps}.
\eq{full-system-first} through \eq{full-system-last} define a banded matrix
equation in the ``extended'' space and, as before, this can be used to solve
for $K^{-1}\,\bvec{r}_\bvec{\theta}$ and $\det K$ in $\mathcal{O}(N)$
operations.
\Figure{matrix} shows a pictorial representation of the sparsity pattern of
the extended matrix $K_\mathrm{ext}$.
Given that definition of $K_\mathrm{ext}$, the corresponding extended vectors
$\bvec{z}_\mathrm{ext}$ and $\bvec{r}_\mathrm{ext}$ are defined schematically
as
\begin{eqnarray}
{\bvec{z}_\mathrm{ext}} ^\T =
\left(\begin{array}{cccccccccc}
    z_1 & u_{1,j} & v_{1,j} & g_{1,j} & h_{1,j} & z_2 & u_{2,j} & \cdots &
    h_{N-1,j} & z_N
\end{array}\right)
\end{eqnarray}
and
\begin{eqnarray}
{\bvec{r}_\mathrm{ext}} ^\T =
\left(\begin{array}{cccccccccc}
    r_{\bvec{\theta},1} & 0 & 0 & 0 & 0 & r_{\bvec{\theta},2} &
    0 & \cdots & 0 & r_{\bvec{\theta},2}
\end{array}\right) \quad.
\end{eqnarray}
After constructing the extended matrix (using a compact storage format), the
extended matrix can be factorized using a LU-decomposition\footnote{Even
though $K_\mathrm{ext}$ is symmetric, it is not positive definite so a
Cholesky solver cannot be used for increased efficiency.} routine optimized
for band or sparse matrices.
This decomposition can then be used to compute the determinant of $K$, solve
$K^{-1}\,\bvec{r}_\bvec{\theta}$, and subsequently calculate the marginalized
likelihood in \eq{gp-likelihood}.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/matrix.pdf}
\caption{A pictorial representation of the sparse extended matrix
$K_\mathrm{ext}$ with $N=5$ and $J=2$.
Each colored block corresponds to a non-zero entry in the matrix as described
in the legend.
    \figurelabel{matrix}}
\end{center}
\end{figure}

\subsection{Implementation considerations \& scaling}

The extended system defined in the previous section is sparse with fewer than
a few percent non-zero entries and band structure.
In this \sectionname, we empirically investigate the performance and scaling
of three different algorithms for solving this extended system:

\begin{enumerate}

\item \texttt{vanilla}: A simple algorithm for computing the LU decomposition
    for banded matrices using Gaussian elimination \citep{Press:1992,
    Press:2007},

\item \texttt{lapack}: The general banded LU decomposition implementation from
    LAPACK\footnote{We use the \texttt{dgbtrf} and \texttt{dgbtrs} methods
    from LAPACK.} \citep{Anderson:1999} using optimized BLAS
    routines\footnote{The experiments below use the Intel Math Kernel Library
    (\project{MKL}): \url{https://software.intel.com/en-us/intel-mkl}.}, and

\item \texttt{sparse}: A general sparse LU solver~--~the \texttt{SparseLU}
    solver from \project{Eigen} \citep{Guennebaud:2010}~--~that exploits the
    sparsity but not the band structure.

\end{enumerate}

\citet{Ambikasaran:2015} found an empirical scaling of $\mathcal{O}(N\,J^2)$
with their method that used the sparse LU decomposition implemented in the
\project{SuperLU} package \citep{Demmel:1999}.
The theoretical scaling for a band LU decomposition is $\mathcal{O}(N\,J^3)$
because the dimension of the extended matrix scales as $N\,J$ and the
bandwidth scales with $J$ \citep{Press:1992, Press:2007}.
We find that, while the \texttt{vanilla} solver scales as expected, the
\texttt{lapack} implementation scales empirically as $\mathcal{O}(N\,J^2)$ and
offers the fastest solves for $J \gtrsim 8$ on all platforms that we tested.

The benchmark experiments shown here were performed on a MacBook Pro with two
2.6~GHz CPUs but we find similar results on a Dell workstation with 16 2.7~GHz
CPUs and running Ubuntu.
\Figure{benchmark} shows how the cost of computing \eq{gp-likelihood} scales
with $N$ and $J$ using the \texttt{vanilla} solver.
As expected theoretically, the scaling is linear in $N$ for all $N$ and cubic
in $J$ for large $J$.
\Figure{benchmark-lapack} and \Figure{benchmark-sparse} are the same plots for
the \texttt{lapack} and \texttt{sparse} solvers respectively.
For each of these optimized solvers, the empirical scaling is
$\mathcal{O}(N\,J^2)$ at the cost of some extra overhead.
Therefore, for $J \lesssim 5$ or $10$ the \texttt{vanilla} solver is more
efficient than the other algorithms.
The real world performance of \celerite\ depends on the specific platform,
hardware, and \project{LAPACK}/\project{BLAS} implementation but we have found
qualitatively similar results across popular platforms and state-of-the-art
libraries.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/benchmark_darwin.pdf}
\caption{A benchmark showing the computational scaling of \celerite\ using the
    \texttt{vanilla} band solver as a function of the number of data points
    and the number of terms.
    \emph{(left)} The cost of computing \eq{gp-likelihood} with a covariance
    matrix given by \eq{celerite-kernel} as a function of the number of data
    points $N$.
    The different colors show the cost for different numbers of terms $J$ as
    listed in the legend.
    To guide the eye, the black line shows linear scaling in $N$.
    \emph{(right)} The same information plotted as a function of $J$ for
    different values of $N$.
    The legend shows the value of $N$ for each color and the black line shows
    quadratic scaling in $J$.
    \figurelabel{benchmark}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/benchmark_darwin_lapack.pdf}
\caption{The same as \Figure{benchmark} but using the \texttt{lapack} solver
    with the \project{MKL} \project{BLAS} library.
    \figurelabel{benchmark-lapack}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/benchmark_darwin_sparse.pdf}
\caption{The same as \Figure{benchmark} but using the \texttt{sparse} solver
    from \project{Eigen} \citep{Guennebaud:2010}.
    \figurelabel{benchmark-sparse}}
\end{center}
\end{figure}

\section{celerite as a model of stellar variations}

\dfmtodo{Add more intro to this section.}

A special case of the \celerite\ model of great physical interest is a
stochastically-driven damped simple harmonic oscillator.
The differential equation for this system is
\begin{equation}
    \left[\frac{\dd^2}{\dd t^2} + \frac{\omega_0}{Q}\,\frac{\dd}{\dd t}
    + \omega_0^2\right]\, y(t) = \epsilon(t)
\end{equation}
where $\omega_0$ is the frequency of the undamped oscillator, $Q$ is the
quality factor of the oscillator, and $\epsilon(t)$ is a stochastic driving
force.
If $\epsilon(t)$ is white noise, the PSD of this process is given by
\citep[for example][]{Anderson:1990}
\begin{equation}\eqlabel{sho-psd}
S(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{S_0\,\omega_0^4}
    {(\omega^2-\omega_0^2)^2 + \omega_0^2\omega^2/Q^2}
\end{equation}
where $S_0$ is proportional to the power at $\omega = \omega_0$, $S(\omega_0)
= \sqrt{2/\pi}\,S_0\,Q^2$.
The power spectrum in \eq{sho-psd} matches \eq{celerite-psd} if
\begin{eqnarray}\eqlabel{sho-complex}
a_j &=& S_0\,\omega_0\,Q \\
b_j &=& \frac{S_0\,\omega_0\,Q}{\sqrt{4\,Q^2-1}} \\
c_j &=& \frac{\omega_0}{2\,Q}\\
d_j &=& \frac{\omega_0}{2\,Q} \sqrt{4\,Q^2-1} \quad,
\end{eqnarray}
for $Q \ge \frac{1}{2}$.
For $0 < Q \le \frac{1}{2}$, \eq{sho-psd} can be captured by a pair of \celerite\
terms with parameters
\begin{eqnarray}\eqlabel{sho-real}
a_{j\pm} &=& \frac{1}{2}\,S_0\,\omega_0\,Q\,\left[ 1 \pm
        \frac{1}{\sqrt{1-4\,Q^2}}\right] \\
b_{j\pm} &=& 0 \nonumber\\
    c_{j\pm} &=& \frac{\omega_0}{2\,Q}\,\left[1 \mp \sqrt{1-4\,Q^2}\right]
    \nonumber\\
d_{j\pm} &=& 0 \quad. \nonumber
\end{eqnarray}

These identities yield a kernel of the form
\begin{equation}\eqlabel{sho-kernel}
k(\tau) = S_0\,\omega_0\,Q\,e^{-\frac{\omega_0\,\tau}{2Q}}\,
\begin{cases}
    \cosh{(\eta\,\omega_0\,\tau)} +
        \frac{1}{2\,\eta\,Q}\,\sinh{(\eta\,\omega_0\,\tau)}, & 0 < Q < 1/2\\
    2\,(1+\omega_0\,\tau), & Q = 1/2\\
    \cos{(\eta\,\omega_0\,\tau)} +
        \frac{1}{2\,\eta\,Q} \sin{(\eta\,\omega_0\,\tau)},& 1/2 < Q\\
\end{cases}
\end{equation}
where $\eta = \vert 1-(4\,Q^2)^{-1}\vert^{1/2}$.
It is interesting to note that, because of the damping, the characteristic
oscillation frequency in this model $d_j$, for any finite quality factor $Q > 1/2$,
is not equal to the frequency of the undamped oscillator $\omega_0$.

The power spectrum in \eq{sho-psd} has several limits of physical interest:
\begin{itemize}

{\item For $Q = 1/\sqrt{2}$, \eq{sho-psd} simplifies to
\begin{eqnarray}\eqlabel{granulation-psd}
S(\omega) = \sqrt{\frac{2}{\pi}}\,\frac{S_0}{(\omega/\omega_0)^4+1} \quad.
\end{eqnarray}
This, in turn, is the most commonly used model for the background granulation
noise in asteoreseismic \citep{Kallinger:2014} and helioseismic
\citep{Harvey:1985, Michel:2009} analyses.
The Fourier transform of this PSD corresponds to the kernel
\begin{equation}
k(\tau) = S_0\,\omega_0\,e^{-\frac{1}{\sqrt{2}}\,\omega_0\,\tau}\,
    \cos{\left(\frac{\omega_0\,\tau}{\sqrt{2}}-\frac{\pi}{4}\right)}.
\end{equation}}

{\item Substituting $Q = 1/2$, \eq{sho-psd} becomes
\begin{eqnarray}
S(\omega) =
    \sqrt{\frac{2}{\pi}}\,\frac{S_0}{\left[(\omega/\omega_0)^2+1\right]^2}
\end{eqnarray}
with the corresponding covariance function (using
\eqalt{celerite-kernel} and \eqalt{sho-real})
\begin{eqnarray}\eqlabel{approx-matern}
k(\tau) &=& \lim_{f \to 0}\,
    \frac{1}{2}\,S_0\,\omega_0\,
    \left[\left(1+1/f\right)\,e^{-\omega_0\,(1-f)\,\tau} +
          \left(1-1/f\right)\,e^{-\omega_0\,(1+f)\,\tau}
    \right] \\
&=& S_0\,\omega_0\,e^{-\omega_0\,\tau}\,[1+\omega_0\,\tau]
\end{eqnarray}
or equivalently (using \eqalt{celerite-kernel} and \eqalt{sho-complex})
\begin{eqnarray}\eqlabel{approx-matern2}
k(\tau) &=& \lim_{f \to 0}\,
    S_0\,\omega_0\,e^{-\omega_0\,\tau}\,
    \left[\cos(f\,\tau) + \frac{\omega_0}{f}\,\sin(f\,\tau)\right] \\
&=& S_0\,\omega_0\,e^{-\omega_0\,\tau}\,[1+\omega_0\,\tau] \quad.
\end{eqnarray}
This covariance function is also known as the Mat\'ern-3/2 function
\citep{Rasmussen:2006}.
This suggests that the Mat\'ern-3/2 covariance can be well approximated using
the \celerite\ framework with a small value of $f$ in \eq{approx-matern2} but we
caution that this could also lead to numerical issues with the solver.
}

{\item Finally, in the limit of large $Q$, the model approaches a high
    quality oscillation with frequency $\omega_0$ and covariance function
\begin{eqnarray}
k(\tau) \approx
    S_0\,\omega_0\,Q\,
    \exp\left(-\frac{\omega_0\,\tau}{2\,Q}\right)\,
    \cos\left(\omega_0\,\tau\right) \quad.
\end{eqnarray}}

\end{itemize}
\Figure{sho} shows a plot of the PSD for these limits and several other values
of $Q$.
From this figure, it is clear that for $Q \le 1/2$, the model has no
oscillatory behavior and that for large $Q$, the shape of the PSD near the
peak frequency approaches a Lorentzian.

These special cases demonstrate that the stochastically-driven simple harmonic
oscillator provides a physically motivated model that is flexible enough to
describe a wide range of stellar variations.
Low $Q \approx 1$ can capture granulation noise and high $Q \gg 1$ is a good
model for asteroseismic oscillations.
In practice, we will take a sum over oscillators with different values of $Q$,
$S_0$, and $\omega_0$ to give a sufficient accounting of the power spectrum
stellar time series.
Since this kernel is exactly described by the exponential kernel, the
likelihood (\eqalt{gp-likelihood}) can be evaluated for a time series with $N$
measurements in $\mathcal{O}(N)$ operations using the \celerite\ method
described in the previous section.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/sho.pdf}
\caption{(left) The power spectrum of a stochastically-driven simple harmonic
    oscillator (\eqalt{sho-psd}) plotted for several values of the quality
    factor $Q$.
    For comparison, the dashed line shows the Lorentzian function from
    \eq{lorentz-psd} with $c_j = \omega_0/2\,Q = 1/20$ and normalized so that
    $S(d_j)/S(0) = 100$.
    (right) The corresponding autocorrelation functions with the same colors.
    \figurelabel{sho}}
\end{center}
\end{figure}


\section{Examples with simulated data}

To demonstrate the application of \celerite, we will start by inferring
posterior constraints on the parameters of a GP model applied to several
simulated datasets with known properties.
In the following section, we will expand on these examples by applying
\celerite\ to real datasets.
In the first example, we demonstrate that \celerite\ can be used to measure
the power spectrum of a process when the data are generated from a \celerite\
model.
In the second example, we demonstrate that \celerite\ can be used as an
effective model even if the true process cannot be represented in the space of
allowed models.
This is a interesting example because, when analyzing real data, we rarely
have any fundamental reason to believe that the data were generated by a GP
model with a specific kernel.
Even in these cases, GPs can be useful effective models and \celerite\
provides computational advantages over other GP methods.

\subsection{Recovery of a celerite process}

In this first example, we will simulate a dataset using a known \celerite\
process and fit it with \celerite\ to demonstrate that valid inferences can be
made in this idealized case.
The simulated dataset is shown in the left panel of \Figure{simulated-correct}
and it was generated using a SHO kernel (\eqalt{sho-kernel}) with parameters
$S_0 = 1$, $\omega_0 = e^2$, and $Q = e^2$.
The true PSD is shown as a dashed line in the right panel of
\Figure{simulated-correct}.
We applied log-uniform priors to all of the parameters and used \emcee\
\citep{Foreman-Mackey:2013} to sample the joint posterior density and computed
the marginalized posterior inference of the PSD.
This inference is shown in the right panel of \Figure{simulated-correct} as a
blue contour indicating 68\% of the posterior mass.
It is clear from this figure that, as expected, the inference correctly
reproduces the true PSD.


\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/simulated/correct.pdf}
\caption{(left) A simulated dataset.
    (right) The inferred PSD~--~the blue contours encompass 68\% of the
    posterior mass~--~compared to the true PSD (dashed black line).
    \figurelabel{simulated-correct}}
\end{center}
\end{figure}


\subsection{Inferences with the ``wrong'' model}

For this example, we simulate a dataset using a known GP model with a kernel
outside of the support of a \celerite\ process.
This means that the true autocorrelation of the process can never be correctly
represented by the model that we're using to fit but we will use this example
to demonstrate that, at least in this case, valid inferences can still be made
about the physical parameters of the model.

For this example, the data are simulated from a quasiperiodic GP with the
kernel
\begin{eqnarray}\eqlabel{sim-wrong-true}
k_\mathrm{true} (\tau) = \alpha\,
    \exp\left(-\frac{\tau^2}{2\,\lambda^2}\right)\,
    \cos\left(\frac{2\,\pi\,\tau}{P_\mathrm{true}}\right)
\end{eqnarray}
where $P_\mathrm{true}$ is the fundamental period of the process.
This autocorrelation structure yields the power spectrum
\begin{eqnarray}
S_\mathrm{true} (\omega) = \frac{\lambda\,\alpha}{2}\,\left[
    \exp\left(-\frac{\lambda^2}{2}\,\left(\omega-
        \frac{2\,\pi}{P_\mathrm{true}}\right)^2\right) +
    \exp\left(-\frac{\lambda^2}{2}\,\left(\omega+
        \frac{2\,\pi}{P_\mathrm{true}}\right)^2\right)
\right]
\end{eqnarray}
that for large $\omega$ falls off exponentially.
When compared to \eq{celerite-psd}~--~that for large $\omega$ goes as
$\omega^{-4}$, at most~--~it is clear that a \celerite\ model can never
perfectly reproduce the structure of this process.
That being said, we will demonstrate that rigorous inferences can be made
about $P_\mathrm{true}$ even with an effective model.
The left panel of \Figure{simulated-wrong} shows the simulated dataset.
We then fit this simulated data using the product of two SHO terms
(\eqalt{sho-kernel}) where one of the terms has $S_0 = 1$ and $Q =
1/\sqrt{2}$ and the other has $\omega_0 = 2\,\pi/P$.
We note that using \eq{product-rule}, the product of two \celerite\ terms can
also be expressed as a \celerite\ term.
We apply log-uniform priors on all the parameters and use \emcee\
\citep{Foreman-Mackey:2013} to sample the posterior probability for all of the
parameters.
The inferred distribution for the parameter $P$ is shown in the right panel of
\Figure{simulated-wrong} and compared to the true period $P_\mathrm{true}$ and
the inferences made using the correct model (\eqalt{sim-wrong-true}).
The inference made using this effective \celerite\ model are indistinguishable
from the inferences made using the correct model but substantially less
computation time is required for the \celerite\ inference.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/simulated/wrong-qpo.pdf}
\caption{(left) A simulated dataset.
    (right) The inferred period of the process. The true period is indicated
    by the vertical orange line, the posterior inference using the correct
    model is shown as the blue dashed histogram, and the inference made using
    the ``wrong'' effective model is shown as the black histogram.
    \figurelabel{simulated-wrong}}
\end{center}
\end{figure}


\section{Examples with real data}

In this section we will demonstrate several use cases of \celerite\ when
applied to real datasets.
Each of these examples touches on an active area of research so we will limit
our examples to be qualitative in nature and make no claim of optimality but
we hope that these examples will encourage interested readers to investigate
the applicability of \celerite\ to their research.

All of the following examples show time domain datasets with a clear bias in
favor of large homogeneous photometric surveys but these methods can similarly
be applied to spectroscopy, where wavelength~--~instead of time~--~is the
independent coordinate and other one-dimensional domains.
It is also possible to cast some two-dimensional problems in the \celerite\
framework (\dfmtodo{CITE Ian C}).

\subsection{Asteroseismic oscillations}

The asteroseismic oscillations of thousands of stars were measured using light
curves from the \kepler\ mission \citep{Gilliland:2010, Chaplin:2011,
Chaplin:2013, Stello:2013} and asteroseismology is a key science driver for
many of the upcoming large scale photometric surveys \citep{Campante:2016,
Rauer:2014, Gould:2015}.
Most asteroseismic analyses have been limited to relatively high
signal-to-noise oscillations because the standard methods based on statistics
of the empirical periodogram of the data cannot be used to formally propagate
the measurement uncertainties to the constraints on physical parameters
(\dfmtodo{CITE}) but more sophisticated methods that compute the likelihood
function in the time domain scale poorly to state-of-the-art datasets
\citep{Brewer:2009}.

\celerite\ alleviates these problems by providing a physically motivated
probabilistic model that can be evaluated efficiently even for large datasets.
In practice, one would model the star as a mixture of stochastically-driven
simple harmonic oscillators where the amplitudes and frequencies of the
oscillations are computed using a physical model and evaluate the probability
of the observed dataset using a GP where the PSD is a sum of terms given by
\eq{sho-psd}.
This gives us a method of computing the likelihood function for the parameters
of the physical model (for example, $\nu_\mathrm{max}$ and $\Delta \nu$)
\emph{conditioned on the observed time series} in $\mathcal{O}(N)$ operations.
In other words, this provides us with a computationally efficient method for
performing rigorous probabilistic inference of asteroseismic parameters in the
time domain.
We expect that this has the potential to push asteroseismic analysis to lower
signal-to-noise datasets and we hope to revisit this idea in a subsequent
paper.

To demonstrate this method, we will use a very simple heuristic model based on
\dfmtodo{CITE} where the model PSD is given by a mixture of 9 components with
amplitudes and frequencies specified by $\nu_\mathrm{max}$, $\Delta \nu$, and
several nuisance parameters.
The first two terms are used to capture the granulation ``background''
\citep{Kallinger:2014} using \eq{granulation-psd} with two free parameters
$S_g$ and $\omega_g$ in each term.
The remaining  7 terms are given by \eq{sho-psd} where $Q$ is a nuisance
parameter shared between terms and the frequencies are given by
\begin{eqnarray}
\omega_{0,\,j} = 2\,\pi\,(\nu_\mathrm{max} + j\,\Delta\nu + \epsilon)
\end{eqnarray}
and the amplitudes are given by
\begin{eqnarray}
S_{0,\,j} =
    \frac{A}{Q^2}\,\exp\left(-\frac{[j\,\Delta\nu + \epsilon]^2}{2\,W^2}\right)
\end{eqnarray}
where $j$ is an integer running from $-3$ to 3 and $\epsilon$, $A$, and $W$
are shared nuisance parameters.
This model could be easily extended to include small frequency splitting and
$\nu_\mathrm{max}$ and $\Delta \nu$ could be replaced by physical parameters
like $\log g$.

To demonstrate the applicability of this model, we apply it to infer the
asteroseismic parameters of the giant star KIC~11615890 observed by the
\kepler\ Mission.
The goal of this example is to show that, even for a low signal-to-noise
dataset with a short baseline, it is possible to infer asteroseismic
parameters with formal uncertainties that are consistent with the parameters
inferred with a much larger dataset.
Looking forward to \tess\ \citep{Campante:2016}, we will infer
$\nu_\mathrm{max}$ and $\Delta\nu$ using only one month of \kepler\ data and
compare our results to the results inferred from the full 4 year baseline of
the \kepler\ mission.
For KIC~11615890, the published asteroseismic parameters based on several
years of \kepler\ observations are \citep{Pinsonneault:2014}
\begin{eqnarray}
    \nu_\mathrm{max} = 171.94 \pm 3.62 \,\mu\mathrm{Hz} \quad\mathrm{and}\quad
    \Delta\nu = 13.28 \pm 0.29 \,\mu\mathrm{Hz} \quad.
\end{eqnarray}
We randomly select a month-long segment of \kepler\ data, initialize our
\celerite\ model using a grid search in the parameter space, and then use
\emcee\ \citep{Foreman-Mackey:2013} to sample the joint posterior density for
the full set of parameters.
\Figure{astero-corner} shows the marginalized density for $\nu_\mathrm{max}$
and $\Delta\nu$ compared to the results from the literature.
\dfmtodo{Add some more discussion here.}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/astero-11615890-numax_deltanu_corner.pdf}
\caption{The probabilistic constraints on $\nu_\mathrm{max}$ and $\Delta \nu$
    from the inference shown in \Figure{astero} compared to the published
    value (error bar) based on the full \kepler\ dataset \dfmtodo{CITE}.
    \figurelabel{astero-corner}}
\end{center}
\end{figure}


\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/astero-11615890-comparisons.pdf}
\caption{A comparison between the empirical PSD and the posterior inference of
the PSD as a mixture of stochastically-driven simple harmonic oscillators.
(top) The periodogram of the \kepler\ light curve for KIC~11615890 computed
    on the full four year baseline of the mission.
    The light gray curve shows the raw periodogram and the black curve has
    been smoothed with a Gaussian filter with \dfmtodo{SOME WIDTH}.
(middle) The same periodogram computed using about a month of data.
(bottom) The power spectrum inferred using the mixture of SHOs model described
    in the text and only one month of \kepler\ data.
    The black line shows the median of posterior PSD and the gray contours
    show the 68\% credible region.
    \figurelabel{astero}}
\end{center}
\end{figure}

\subsection{Stellar rotation}

Another source of variability that can be measured from time series
measurements of stars is rotation.
The inhomogeneous surface of the star (spots, plage, \etc) imprints itself as
quasiperiodic variations in photometric or spectroscopic observations
(\dfmtodo{CITE}).
It has been demonstrated that for light curves with nearly uniform sampling,
the empirical autocorrelation function provides a reliable estimate of the
rotation period of a star (\dfmtodo{CITE}) and that a GP model with a
quasiperiodic covariance function can be used to make probabilistic
measurements even with sparsely sampled data (R.~Angus, \etal\ in prep.).
The covariance function used for this type of analysis has the form
\begin{eqnarray}\eqlabel{sine2}
k(\tau) = A\,\exp\left(-\frac{\tau^2}{2\,\ell^2} -
    \Gamma\,\sin^2\left(\frac{\pi\,\tau}{P} \right) \right)
\end{eqnarray}
where $P$ is the period of the oscillation.
The key difference between this function and other quasiperiodic kernels is
that it is everywhere positive.
We can construct a simple \celerite\ covariance function with similar properties
as follows
\begin{eqnarray}\eqlabel{rot-kernel}
k(\tau) = \frac{a}{2+b}\,e^{-c\,\tau}\,\left[
    \cos\left(\frac{2\,\pi\,\tau}{P}\right) + (1 + b)
\right]
\end{eqnarray}
for $a>0$, $b>0$, and $c>0$.
The covariance function in \eq{rot-kernel} cannot exactly reproduce \eq{sine2}
but, since \eq{sine2} is only an effective model, \eq{rot-kernel} can be used
as a drop-in replacement for a substantial gain in computational efficiency.

To demonstrate this model, we fit a \celerite\ model with a kernel given by
\eq{rot-kernel} to a \kepler\ light curve for the star KIC~1430163.
This star has a published rotation period of $3.88 \pm 0.58$ measured using
traditional periodogram and autocorrelation function approaches applied to
\kepler\ data from Quarters 0--16 \citep{Mathur:2014}.
We used \emcee\ to sample to posterior probability density for the four
parameters in \eq{rot-kernel} conditioned on two quarters of \kepler\ data and
find a constraint on the rotation period of $P = \rotationperiod$ in good
agreement with the literature value.
\Figure{rotation} shows a subset of the data used in this example and the
posterior inferences of the PSD and autocorrelation function of the process.
\Figure{rotation-period} shows the marginalized posterior distribution for the
rotation period.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/rotation.pdf}
\caption{Inferred constraints on a quasiperiodic GP model using the covariance
    function in \eq{rot-kernel} and two quarters of \kepler\ data.
(left) The \kepler\ data (black points) and the maximum likelihood model
    prediction (blue curve) for a 50~day subset of the data used.
    The solid blue line shows the predictive mean and the blue contours show
    the predictive standard deviation.
(center) Inferred constraints on the model PSD.
    The dashed line shows the maximum likelihood PSD, the blue solid line
    shows the median of posterior PSD, and the blue contours show the 68\%
    credible region.
(right) Inferred constraints on the model covariance function from
    \eq{rot-kernel}.
    The dashed line shows the maximum likelihood model, the blue solid line
    shows the median of posterior, and the blue contours show the 68\%
    credible region.
    \figurelabel{rotation}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/rotation-period.pdf}
\caption{The posterior constraint on the rotation period of KIC~1430163 using
    the dataset and model from \Figure{rotation}.
    The period is the parameter $P$ in \eq{rot-kernel} and this figure shows
    the posterior distribution marginalized over all other nuisance parameters
    in \eq{rot-kernel}.
    This is consistent with the published rotation period made using the
    autocorrelation function and the full \kepler\ data (\dfmtodo{CITE}).
    \figurelabel{rotation-period}}
\end{center}
\end{figure}

\subsection{Exoplanet transit fitting}

In this example, we inject the signal of a simulated exoplanet transit into a
real \kepler\ light curve and then demonstrate that we can recover the true
physical parameters of the exoplanet while modeling the stellar variability
using \celerite.
This example is different from all the previous examples because in this case,
we are uninterested in the inferred parameters of the covariance model.
Instead, we're interested in inferring constraints on the parameters of the
mean model.
In \eq{gp-likelihood} these parameters are called $\bvec{\theta}$ and in this
example, the mean function $\mu_\bvec{\theta}(t)$ is a limb-darkened transit
light curve \dfmtodo{CITE MA} parameterized by a period $P$, a transit
duration $T$, a phase or epoch $t_0$, an impact parameter $b$, the radius
of the planet in units of the stellar radius $R_P/R_\star$, and several
parameters describing the limb-darkening profile of the star \dfmtodo{CITE}.
As in the previous example, we model the stellar variability using a GP model
with a kernel given by \eq{rot-kernel}.

To demonstrate this example, we take a month-long segment of the \kepler\
light curve for KIC~1430163~--~the target from the previous example~--~and
multiply it by a simulated transit model with known parameters.
The top panel of \Figure{transit-ml} shows the data including the simulated
transit.
The bottom panel shows the same data with the maximum likelihood prediction
for the stellar variation subtracted.
To find this model, we maximized the likelihood function in \eq{gp-likelihood}
with respect to the transit parameters $\bvec{\theta}$ and the variability
parameters $\bvec{\alpha}$ simultaneously.
\Figure{transit-corner} shows the marginalized posterior constraints on the
physical properties of the planet compared to the true values.
This procedure produces estimates of the planet parameters that are consistent
with the true values using \celerite\ as an effective model for the stellar
variability.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/transit-ml.pdf}
    \caption{\emph{(top)} A month-long segment of \kepler\ light curve for
    KIC~1430163 with a synthetic transit model injected (black points) and the
    maximum likelihood model for the stellar variability and the transit model
    (blue line).
    \emph{(bottom)} The maximum likelihood ``de-trendeding'' of the data in
    the top panel.
    In this panel, the maximum likelihood model for the stellar variability
    has been subtracted to leave only the transits.
    The de-trended light curve is shown by black error bars and the maximum
    likelihood transit model is shown as a blue line.
    \figurelabel{transit-ml}}
\end{center}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/transit-corner.pdf}
\caption{The marginalized posterior constraints on the physical parameters of
    the planet transiting during the light curve shown in the top panel of
    \Figure{transit-ml}.
    The two-dimensional contours show the 0.5-, 1-, 1.5, and 2-sigma credible
    regions in the marginalized planes and the histograms along the diagonal
    show the marginalized posterior for each parameter.
    The true values used in the simulation are indicated by blue lines.
    For each parameter, the inference is consistent with the true value.
    \figurelabel{transit-corner}}
\end{center}
\end{figure}

\section{Comparisons to other methods}

Toeplitz, KISS-GP, CARMA, HODLR.

Limitations of celerite: one-dimension, stationary, etc.



% DFM: I'm going to redistribute the following in other sections.

% Stars are variable, for better or worse:  their variability provides information about the properties
% of stars, but also results in noise that must be accounted for in detecting and characterizing their
% planetary systems.  When analyzing astronomical times series of stars, then, there are generally two
% goals:  1). to analyze the frequency spectrum and amplitude of stellar variability to better understand stellar
% properties and evolution, and/or 2). to account for correlated noise when modeling the stellar
% flux variability and spectral variations.

% A promising technique for addressing both of these goals is the Gaussian Process formalism
% \citep{Rasmussen2006,Gibson:2012}.  The basis of this method is to assume that stellar variability
% is random, Gaussian noise, but that the random variability is correlated in time, and the nature of the
% correlations is completely specified by a correlation matrix.  In its simplest form, the
% correlation matrix is modeled with a time-independent autocorrelation function (although
% the time-independence can be relaxed), including a white-noise component that may vary
% with each exposure, and the autocorrelation function is described by a functional form that may be
% simply parameterized by a relatively small number of parameters, sometimes referred to as
% `hyper-parameters.'  The goal of spectral analysis is to measure the parameters which
% describe the autocorrelation function, thus inferring properties of the stellar variabilty
% which may be used to characterize properties of the star, such as the rotation period,
% the asteroseismic frequencies, or granulation noise known as `flicker'
% \citep{Aerts2010,Noyes:1984,Bastien:2013}.
% The goals of planet detection include measuring the Doppler shift of the star, measuring
% the decrement of flux as a planet transits a star, and/or measuring the times of transit to
% look for dynamical interactions between planets.  To properly account for the impact of
% stellar variability on the planet parameters requires treating correlations in the data, and
% to do so also requires evaluating the correlation matrix.

% The size of the correlation matrix, $N_{elements}$, scales as the square of the length of the time-series,
% $N_{element}=N_{time}^2$, so as time series grow in size, say $N_{time} =10^{5-6}$, the storage may become
% prohibitive, $N_{element} = 10^{10-12}$.  In addition, this matrix must be inverted and have
% its determinant evaluated to compute the likelihood function, which takes of order $N_{operations}
% = N_{time}^3 = 10^{15-18}$.  This becomes an impossible computational problem, which prohibits
% using long time series datsets, such as the {\it Kepler} dataset $N_{time} \approx 10^{5-6}$,
% {\it Spitzer} time series with $N_{time} \approx 10^5$, and Solar time series with
% $N_{time} \approx 10^7$.  So a faster means of solving for the likelihood is required in these
% cases.

% One promising solution is to use approximate techiques based upon the fact that correlation
% matrices display a great deal of symmetry.  This `hierarchical off-diagonal low-rank' (\texttt{HODLR})
% algorithm can make the storage and operations both scale in proportion to $N_{time}$, although the
% solver is only approximate \citep{Ambikasaran:2013,Ambikasaran:2016}.  In addition, the
% solver is complicated and can be challenging to use with general choices of kernel.  Another
% promising approximate technique is to use a uniform resampling of the dataset, and then
% rely on the structure of Toeplitz matrices, which require uniformly space time series,
% to obtain approximate solutions of the likelihood function \citep{Wilson:2015},
% the `Kernel Interpolation for Scalable Structured Gaussian Processes' (\texttt{KISS-GP});
% this approach has the drawback of being approximate.  A third approach is to use a kernel
% based upon stochastic differential equations:  the `continuous auto-regressive moving-average'
% model \citep[aka \texttt{CARMA};][]{Kelly:2014}.  This approach can model a stationary power-spectrum that
% is the combination of a number of Lorentzians, but has the disadvantage that the amplitudes
% of the Lorentzians are required to have a specific relation.  In addition, these
% techniques require a stationary kernel. {\color{red} Is this true for GEORGE?}

% The approach we explore in this paper is based upon a method developed by Press \&
% Rybicki twenty years ago \citep{Rybicki:1995}.  They observed that a correlation
% matrix consisting of a single exponential kernel has an inverse that is tri-diagonal.
% The coefficients and solution of a tri-diagonal matrix scale in proportion to $O(N_{time})$.
% They also indicated that a combination of two exponential kernels would have a simliar scaling,
% but they did not demonstrate how to extend this to more than two kernel components.

% A significant advance was made in generalizing the Press-Rybicki method to an
% arbitrary number of exponential kernels by utilizing the fact that the elements of
% such matrices can be written in terms of the products of components of two vectors,
% which are so-called semi-separable matrices \citep{Ambikasaran:2015}.  This `Generalized
% Rybicki-Press' (\texttt{celerite}) approach
% has many advantages:  the computation time and storage both scale as $O(N_{time})$,
% the coefficients of the kernels need not be stationary, the computation of the
% likelihood is exact, and the algorithm is simple to implement, and thus is robust.
% Unlike the \texttt{CARMA} model, there is no restriction on the relation between
% the coefficients (except that the kernel needs to be positive definite).  Unlike
% \texttt{HODLR} and \texttt{KISS-GP},  the \texttt{celerite} method is exact.  The main drawback
% of this approach is that the sum of exponential kernels with real coefficients can
% only represent a limited range of power spectra of auto-correlated time series:
% those which can be expressed the sum of Lorentzians that peak at zero frequency.

% However, this drawback turns out to not be significant:  the exponential kernel
% can also have a complex exponential coefficients (plus their complex-conjugates), and
% thus approximate an almost arbitrary autocorrelation function with enough terms, most
% importantly allowing for quasi-periodic kernels.  It turns out that the sum of Lorentzian
% components is a {\it very} good approximation of stellar variability spectra:  the
% activity and granulation noise due to stellar convection may be expressed in terms of zero-frequency
% Lorentzians \citep{Harvey:1985}, the asteroseismic oscillations may be expressed as non-zero frequency
% Lorentzians \citep{1990ApJ...364..699A,Gruberbauer:2009}, and stellar rotation may be
% parameterized this way as well.  In this paper
% we extend the \texttt{celerite} approach to kernels with complex exponentials, which when
% combined with the complex conjugate leads to damped harmonic auto-correlation functions
% which are flexible enough to describe a wide range of variability, and may still
% be solved in $O(N_{time}$ operations.

% We show that the extended matrix that embeds this multi-component kernel can be written
% in real notation, which speeds up the evaluation of the likelihood function,
% and allows optimization software to use automatic differential in computing
% derivatives of the likelihood function.

\section{Summary}

Although we have in mind applications of this fast method to model stellar
variability, the method is general for one-dimensional GP problems, and may be
applied to other problems.
Accreting black holes show time series which may be modeled by correlated
noise \citep{Kelly:2014};  indeed, this was the motivation for the original
technique developed by Rybicki \& Press \citep{Rybicki:1992,Rybicki:1995}.
This approach may be broadly used for characterizing quasar variability
\citep{MacLeod:2010}, measuring time lags with reverberation mapping
\citep{Zu:2011,Pancoast:2014}, and modeling time delays in multiply-imaged
gravitationally-lensed systems \citep{Press:1998}.

Outside of astronomy, this technique may have application to seismology
\citep{Robinson:1967},

There are three primary applications we envision for the \celerite\ formalism:
1).\ modeling the variability of an astrophysical system to infer it
properties; 2).\ accounting for astrophysical variability as a source of noise
when trying to detect additional phenomena; 3).\ interpolating or
extrapolating variability to future or missing times.
Some examples of the first are determining the characteristic variability
timescale of a quasar \citep{Kelly:2014}, measuring the asteroseismic
variation of a star \citep{Brewer:2009,Corsaro:2014}, measurement of
granulation noise \citep{Bastien:2013,Kallinger:2016}, detection of
quasi-periodic variability in a high-energy source \citep{McAllister:2016}, or
classification of variable objects \citep{Zinn:2016}.
Examples of the second are detecting and characterizing transiting exoplanets
\citep[for example,][]{Gibson:2012, Barclay:2015, Evans:2015, Aigrain:2016,
Grunblatt:2016, Luger:2016}
and correcting for stellar activity in radial velocity measurements
\citep[for example,][]{Haywood:2014, Rajpaul:2015}.
Examples of the third are measuring time delays in multiply-imaged
gravitationally lensed sources \citep{Tewes:2013} and reverberation mapping of
AGN.

Our background is in studying transiting exoplanets, a field which has only
recently begun to adopt full covariance matrices in analyzing the noise in
transiting planet lightcurves.
One promising early attempt at accounting for correlated noise was the wavelet
approach of \citet{Carter:2009}.
Their technique allowed for a power-law spectrum for the noise scaling as
$\omega^{-\alpha}$, which unfortunately is not a normalizable, nor
physically-accurate, description of stellar variability.
Their approach runs in ${\cal O}(N)$ time for $\alpha=1$, which is referred to
as `flicker', $1/r$, or pink noise;  however, this particular type of noise
overpredicts the power spectrum at low frequency compared to the Solar
granulation power spectrum.
It also cannot describe quasi-periodic noise.

Further progress was made by parameterizing the covariance matrix with simple,
analytic functions that describe the autocorrelation function.  The parameters
of these functions can then be optimized to best match the observed variability
pattern of the residuals (after subtracting a transit model).  The disadvantage of
this approach is that the functions used are chosen somewhat arbitrarily (e.g.\
the exponential-squared function), and the Cholesky decomposition of the covariance
matrix for computing the likelihood takes ${\cal O}(N^3)$ operations which prohibits
application of this technique to large datasets.

Given the drawbacks of these approaches, the \celerite\ formalism allows both a fast
computation of the likelihood in ${\cal O}(N)$ time, as well as a functional form
that accurately describes stellar variability due to the relation to the simple
harmonic oscillator power spectrum, which is an analog of stellar asteroseismic
oscillations.  As higher signal-to-noise observations of transiting exoplanet systems
are obtained, the effects of stellar variability will more dramatically impact the
correct inference of planetary transit parameters, and so we expect that \celerite\
will be important for transit timing, transit spectroscopy, Doppler beaming,
phase functions, and more.

In the future we hope to expand the \celerite\ technique to multi-dimensional
datasets.  The formalism presented could, for example, be used to model
the covariance between multiple measured quantities, such as in regression of photometric
time series versus astrometric precision when correcting for systematic errors
due to detector sensitivity variations \citep{Aigrain:2016}, in correcting
radial velocity datasets for stellar activity \citep{Haywood:2014,Rajpaul:2015},
or in detection of gravitational waves in the presence of systematic errors
\citep{Moore:2016}.

%\section{Prediction/interpolation}
%
%Precition/interpolation involves constructing a matrix of covariances
%between the (noisy) training set and the times at which we wish to
%predict or interpolate data points.   If our training times are given by
%${\bf t}$, while the predicted times are given by ${\bf t}_*$, then
%mean values at the predicted times are given by (RW 2.23):
%\begin{eqnarray}
%\bar f_{*,k} &=& \sum_j k(t_{*,k},t_j) b_j,\\
%&=& \sum_j \sum_p \alpha_p e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
%&=& \sum_p \alpha_p \sum_j e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
%y_k &=& \sum_j k(t_j,t_k) h_j,
%\end{eqnarray}
%where the latter equation may be solved for ${\bf h}$ using the extended
%matrix formalism.  Let the length of ${\bf t}_*$ be $M$; then, the
%matrix $k({\bf t}_*,{\bf t})$ has a size $M \times N$, and so a total
%of $MN$ multiplications is required to obtain the predicted values.
%If $M$ is large, this can become quickly prohibitive.  It turns out
%that the structure of the sum of exponential kernels may be exploited
%to obtain the predicted mean in of order $J(M+N)+J^2N$ multiplications,
%i.e.\ a linear scaling with the total number of data points, which can
%be much fewer if $M$ is large.
%
%The procedure works as follows:
%\begin{itemize}
%\item Sort ${\bf t}$ and ${\bf t}_*$ in time order.
%\item Starting with the smallest value of ${\bf t}_*$, $t_{*,1}$,
%compute:
%\begin{eqnarray} \label{prediction}
%\bar f_{*,k} &=& \sum_p \alpha_p \left[\epsilon^+_{p,k} + \epsilon^-_{p,k}\right]\\
%\epsilon^+_{p,k} &=& \sum_{j \forall t_{*,k} > t_j} e^{-\beta_p (t_{*,k}-t_j)} h_j\\
%\epsilon^-_{p,k} &=& \sum_{j \forall t_{*,k} \le t_j} e^{-\beta_p (t_j-t_{*,k})} h_j,
%\end{eqnarray}
%where we have separated out the training times before and after the predicted time $t_{*,k}$.
%
%\item Next, we update the coefficients recursively:
%\begin{eqnarray}
%\epsilon^+_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^+_{p,k-1} + \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_{*,k}-t_j)} h_j\right]\\
%\epsilon^-_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^-_{p,k-1} - \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_j-t_{*,k})} h_j\right].
%\end{eqnarray}
%With these updates, we can then use equation \ref{prediction} to compute the expected
%mean at $t_{*,k}$ with an additional $J$ operations.  Since there are $N$ values of
%$t_k$, the number of operations needed to update $\epsilon$ is of order $J(N+M)$, and
%so this procedure avoids a heavy computational burden.
%
%The variance may be handled in a similar manner, but with $J^2$ coefficients
%to recursively update. For example, if $t_{*,k} > t_j \forall j,k$ then:
%\begin{eqnarray}
%cov(f_{*,k}) &=&  k(t_{*,k},t_{*,k}) - \sum_p \sum_q \alpha_p \alpha_q e^{-\beta_p(t_{*,k}-{\bf t}^T)} k({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
%&=& k(0) - \sum_p \sum_q \alpha_p \alpha_q \zeta_{p,q,k},\\
%\zeta_{p,q,k} &=&   e^{-\beta_p(t_{*,k}-{\bf t}^T)} k({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
%&=&  e^{-(\beta_p+\beta_q)(t_{*,k}-t_{*,k-1})} \zeta_{p,q,k-1}.
%\end{eqnarray}
%The recursive updates to the $\zeta_{p,q,k}$ parameters can be made at each step.
%\end{itemize}
%
%Problem:  if we have the case that the datapoints are interspersed, and if the sign changes,
%then we need to resolve $K^{-1} e^{-\beta_q \vert t_{*,k}-{\bf t}\vert}$.  However, maybe we can
%just solve for the difference in fewer operations? {\bf TBD}

\vspace{1.5em}
All of the code used in this project is available from
\url{https://github.com/dfm/celerite} under the MIT open-source software
license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).

\acknowledgments
It is a pleasure to thank
Sivaram Ambikasaran,
Megan Bedell,
Will Farr,
Sam Grunblatt,
David W.\ Hogg, and
Dan Huber
for helpful contributions to the ideas and code presented here.

EA acknowledges support from NASA grants NNX13AF20G, NNX13AF62G, and
NASA Astrobiology Institute's Virtual Planetary Laboratory, supported
by NASA under cooperative agreement NNH05ZDA001C.

This research made use of the NASA \project{Astrophysics Data System} and the
NASA Exoplanet Archive.
The Exoplanet Archive is operated by the California Institute of Technology,
under contract with NASA under the Exoplanet Exploration Program.

This paper includes data collected by the \kepler\ mission. Funding for the
\kepler\ mission is provided by the NASA Science Mission directorate.
We are grateful to the entire \kepler\ team, past and present.

These data were obtained from the Mikulski Archive for Space Telescopes
(MAST).
STScI is operated by the Association of Universities for Research in
Astronomy, Inc., under NASA contract NAS5-26555.
Support for MAST is provided by the NASA Office of Space Science via grant
NNX13AC07G and by other grants and contracts.

\facility{Kepler}
\software{%
     \project{corner.py} \citep{Foreman-Mackey:2016},
     \project{Eigen} \citep{Guennebaud:2010},
     \project{emcee} \citep{Foreman-Mackey:2013},
     \project{george} \citep{Ambikasaran:2016},
     \project{LAPACK} \citep{Anderson:1999},
     \project{matplotlib} \citep{Hunter:2007},
     \project{numpy} \citep{Van-Der-Walt:2011},
     \project{transit} \citep{Foreman-Mackey:2016a},
     \project{scipy} \citep{Jones:2001}.
}

\appendix

\section{Ensuring positive definiteness: Sturm's theorem}

The power spectrum is computed from taking the square of the Fourier transform
of the data time series; hence, the power-spectrum must be non-negative.  In
constructing a kernel, if any of the parameters $a_j$ or $b_j$ are negative,
then it is possible for the power spectrum to go negative, which violates the
definition of the power spectrum.
Consequently, if any of these coefficients is negative, it is necessary to
check that the power spectrum is still non-negative.  Note that a non-negative
power-spectrum does not require the auto-correlation
function to be positive for all $\tau$.  The positive power spectrum is
related to the positive eigenvalues of the covariance matrix which are
necessary for a positive-definite matrix in limiting cases
\citep{Messerschmitt:2006}.  We find empirically that requiring an everywhere
positive power spectrum results in positive eigenvalues for the covariance
matrix, and so we describe here how to ensure a positive power spectrum using
Sturm's theorem.
%Relation of positive power spectrum to positive definite covariance matrix.
%Power spectrum of a sum of kernels.

In the case of $J$ \celerite\ terms, we can check for negative values of the
PSD by solving for the roots of the power spectrum (see \eqalt{celerite-psd}),
abbreviating with $z = \omega^2$:
\begin{equation}
S(\omega)=  \sum_{j=1}^J \frac{q_j z + r_j}{z^2+s_jz + t_j} = 0
\end{equation}
where
\begin{eqnarray}
q_j &=& a_jc_j-b_jd_j\\
r_j &=& (d_j^2+c_j^2)(b_jd_j+a_jc_j)\\
s_j &=& 2(c_j^2-d_j^2)\\
t_j &=& (c_j^2+d_j^2)^2.
\end{eqnarray}
The denominators of each term are positive, so we can multiply through by
$\prod_j \left(z^2+s_jz + t_j\right)$ to find
\begin{equation}
Q_0(z) = \sum_{j=1}^J (q_j z + r_j)\prod_{k \ne j}\left(z^2+s_kz +
    t_k\right) = 0\quad,
\end{equation}
which is a polynomial with order $2\,(J-1)+1$.
With $J=2$, this yields a cubic equation which may be solved exactly for the
roots.

For arbitrary $J$, a procedure based upon Sturm's theorem \citep{Dorrie:1965}
allows one to determine whether there are any real roots within the range
$(0,\,\infty]$.
We first construct $Q_0(z)$ and it's derivative $Q_1(z) = {Q_0}^\prime(z)$,
and then loop from $k=2$ to $k=2\,(J-1)+1$, computing
\begin{eqnarray}
Q_k(z) = -{\rm rem}(Q_{k-2},\,Q_{k-1})
\end{eqnarray}
where the function ${\rm rem}(p,\,q)$ is the remainder polynomial after
dividing $p(z)$ by $q(z)$.

We evaluate the coefficients of each of the polynomial in the series by
evaluating $f_0 = \{Q_0(0),\,\ldots,\,Q_{2\,(J-1)+1}(0)\}$ to give us the signs
of these polynomials evaluated at $z=0$.
Likewise, we evaluate the coefficients of the largest order term in each
polynomial which gives the sign of the polynomial as $z \rightarrow \infty$,
$f_\infty$.
With the series of coefficients $f_0$ and $f_\infty$, we then determine how
many times the sign changes in each of these, where $\sigma(0)$ is the number
of sign changes at $z=0$, and $\sigma(\infty)$ is the number of sign changes
at $z \rightarrow \infty$.
The total number of real roots in the range
$(0,\,\infty]$ is given by $N_{+}=\sigma(0)-\sigma(\infty)$.

We have checked that this procedure works for a wide range of parameters, and
we find that it robustly matches the number of positive real roots which we
evaluated numerically.
The advantage of this procedure is that it does not require computing the
roots, but only carrying out algebraic manipulation of polynomials to
determine the number of positive real roots.
If a non-zero real root is found, the likelihood may be set to zero.

\bibliography{celerite}

\end{document}
