% Copyright 2015-2016 Dan Foreman-Mackey and the co-authors listed below.

\documentclass[manuscript, letterpaper]{aastex6}

\pdfoutput=1

\include{vc}
\usepackage{microtype}

\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% ----------------------------------- %
% start of AASTeX mods by DWH and DFM %
% ----------------------------------- %

\setlength{\voffset}{0in}
\setlength{\hoffset}{0in}
\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\headheight}{0ex}
\setlength{\headsep}{\baselinestretch\baselineskip} % this is 2 lines in ``manuscript''
\setlength{\footnotesep}{0in}
\setlength{\topmargin}{-\headsep}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}

\linespread{0.54} % close to 10/13 spacing in ``manuscript''
\setlength{\parindent}{0.54\baselineskip}
\hypersetup{colorlinks = false}
\makeatletter % you know you are living your life wrong when you need to do this
\long\def\frontmatter@title@above{
\vspace*{-\headsep}\vspace*{\headheight}
\noindent\footnotesize
{\noindent\footnotesize\textsc{\@journalinfo}}\par
{\noindent\scriptsize Preprint typeset using \LaTeX\ style AASTeX6 with modifications
}\par\vspace*{-\baselineskip}\vspace*{0.625in}
}%
\makeatother

% Section spacing:
\makeatletter
\let\origsection\section
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}
\newcommand\nostarsection[1]{\sectionprelude\origsection{#1}}
\newcommand\starsection[1]{\sectionprelude\origsection*{#1}}
\newcommand\sectionprelude{\vspace{1em}}
\let\origsubsection\subsection
\renewcommand\subsection{\@ifstar{\starsubsection}{\nostarsubsection}}
\newcommand\nostarsubsection[1]{\subsectionprelude\origsubsection{#1}}
\newcommand\starsubsection[1]{\subsectionprelude\origsubsection*{#1}}
\newcommand\subsectionprelude{\vspace{1em}}
\makeatother

\widowpenalty=10000
\clubpenalty=10000

\sloppy\sloppypar

% ------------------ %
% end of AASTeX mods %
% ------------------ %

% Projects:
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\kepler}{\project{Kepler}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\sectionname}{Section}
\newcommand{\sectref}[1]{\ref{sect:#1}}
\newcommand{\Sect}[1]{\sectionname~\sectref{#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\sectref{#1}}
\newcommand{\App}[1]{Appendix~\sectref{#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\agoltodo}[1]{\todo{Agol}{blue}{#1}}


% \shorttitle{}
% \shortauthors{}
% \submitted{Submitted to \textit{The Astrophysical Journal}}

\begin{document}

\title{%
Fast and scalable Gaussian process modeling of stellar variability
\vspace{-3\baselineskip}  % OMG AASTEX6 IS SO BROKEN
}

\newcounter{affilcounter}
% \altaffiltext{1}{}

\setcounter{affilcounter}{1}
\edef \uw {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\uw}       {Astronomy Department, University of Washington,
                          Seattle, WA, 98195, USA}

\edef \sagan {\arabic{affilcounter}}\stepcounter{affilcounter}
\altaffiltext{\sagan}{Sagan Fellow}

\author{%
    Daniel~Foreman-Mackey\altaffilmark{\uw,\sagan} and
    Eric~Agol\altaffilmark{\uw}
}



\begin{abstract}

We derive a fast, $O(N)$, Gaussian Process approach that involves Lorentzian
kernels.  This basis set can approximate commonly used kernels, and is a good
description of stellar variability.

\end{abstract}

\keywords{%
% methods: data analysis
% ---
% methods: statistical
% ---
% catalogs
% ---
% planetary systems
% ---
% stars: statistics
}

\section{Introduction}

Stars are variable, for better or worse:  their variability provides information about the properties
of stars, but also results in noise that must be overcome in detecting and characterizing their
planetary systems.  When analyzing astronomical times series of stars, then, there are generally two 
goals:  1). to analyze the frequency spectrum of stellar variability to beter understand stellar
properties and evolution, and/or 2). to account for correlated noise when modeling the stellar
flux variability and spectral variations.

A promising technique for addressing both of these goals is the Gaussian Process formalism 
\citep{Rasmussen, Gibson}.  The basis of this method is to assume that stellar variability
is random, but that the random variability is correlated in time, and the nature of the
correlations is completely specified by a correlation matrix.  In its simplest form, the 
correlation matrix is modeled with a time-independent autocorrelation function (although 
the time-independence can be relaxed), including a white-noise component that may vary
with time, and the autocorrelation function is described by a functional form that may be 
simply parameterized by a relatively small number of parameters, sometimes referred to as 
`hyper-parameters.'  The goal of spectral analysis is to measure the parameters which
describe the autocorrelation function, thus inferring properties of the stellar variabilty
which may be used to characterize properties of the star, such as the rotation period,
the asteroseismic frequencies, or the `flicker' \citep{Aerts2010,Noyes1984,Bastien2013}.


\section{Outline}

\begin{enumerate}
\item Non-parametric modeling of time series with correlated noise using Gaussian processes
\item Assumptions:\\
  a). Stationary noise (although this can be broken).\\
  b). Gaussian
\item Problem: can't handle large datasets - e.g. entire Kepler long-cadence; Spitzer light
   curves; Solar curves; short-cadence.
\item Solution(s):\\
  a). HODLR \citep{Ambikasaran2013,Ambikasaran2016}\\
  b). CARMA (Lorentzian power spectra, but with limitations) \citep{Kelly2014}\\
  c). Tri-diagonal (Press \& Rybicki) \citep{1995PhRvL..74.1060R}
  d). KISS-GP \citep{WilsonNickisch2015} %Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) Andrew Wilson, Hannes Nickisch Proceedings of The 32nd International Conference on Machine Learning, pp. 1775-1784, 2015
\item Problem: HODLR is tricky to use; CARMA requires certain relations between
  coefficients of Lorentzians (right?); P\&R only works for up to 2 components + white noise
\item Solution: Ambikasaran \citep{Ambikasaran2015} shows how sum of exponential kernels can be solved in order
   $O(N*p^2)$, where p is number of Lorentzians.
  - Sum of Lorentzians is a good description of stellar variability:
     - granulation has expoential (or sums of exponential) noise
     - asteroseismic obeys sums of Lorentzians
     - periodic variability due to spots/plages/etc. can also be
       approximated by Lorentzian(s)
\item Generalized Ambikasarn for complex coefficients (Hermitian).
\item Performance:\\
  a). Show $O(N*p^2)$ scaling\\
  b). Show comparison to HODLR.\\
  c). Show comparison to wavelet (ala Carter \& Winn).\\
  d). Compare with CARMA Kallman filter(?).
\item Various methods/formulae:\\
  a). Likelihood computation.\\
  b). Generating GP with particular covariance [yet to solve]. [ ]\\
  c). Inferring particular components.\\
  d). Computing derivatives of GPs (as used in RV methods).\\
  e). Forecasting/interpolating.\\
  f). Inferring confidence intervals.\\
  g). Derivative of likelihood function with respect to kernel parameters.\\
  h). Non-stationary GP (can the coefficients vary with time?).\\
  i). Multi-band (each element of extended matrix becomes a matrix).
  j). Approximating various kernels: Matern, exp(sin^2), Gaussian, etc.
\item Generalization to non-stationary noise.
\item Example application: TYC 3559\\
  a). Entire long-cadence 17-quarter Kepler dataset (~65k data points).\\
  b). Analysis of solar data.
  c). CO2 Mauna Kea
\item Possible future applications:\\
  a). Inference of asteroseismic variability - ala Andrew Gordon Wilson \citep{2013arXiv1302.4245W}, but
      with Lorentzians, not Gaussians.\\
  b). Multi-waveband GPs.\\
  c). RV interpolation.\\
  d). Non-parametric phase functions.\\
  e). Better calibration of the flicker log(g) relation.\\
  f). Measurement of rotational periods - gyrochronology.\\
  g). Doppler shifts in oscillating EBs.

\item Appendix:
  a). Description/summary of mathematics.\\
  b). Description of code.
\end{enumerate}

\subsection{Derivative of likelihood with respect to Kernel parameters}

The derivative of the likelihood can be computed in $O(N)$ operations as follows.

The derivative of the log likelihood with respect to model parameters ${\bf x}$ is given by:
\begin{equation}
    \frac{\partial \ln{\cal L}}{\partial x_i} = (\ln{\cal L})^\prime = -\frac{1}{2}\sum_i \frac{K_{ii}^\prime}{K_{ii}}
    - \frac{1}{2} {\bf y}^T {\bf K}^{-1} {\bf K}^\prime {\bf K}^{-1} {\bf y},
\end{equation}
where $^\prime$ means the derivative wrt $x_i$.
The kernel may be written as:
\begin{equation}
    K_{ij}({\bf \alpha},{\bf \beta}) = \left\{
    \begin{array}{cc}
       w_i + \sum_{l=1}^p \alpha_l & j = i \\
       \sum_{l=1}^p \alpha_l \exp{(-\beta_l |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.
\end{equation}
Since this equation is the sum of $p$ semi-separable matrices, then applying the product rule to compute the derivative will give a single semi-separable matrix for the derivative with respect to each kernel parameter.

Now, letting ${\bf x} = (w,{\bf \alpha},{\bf \beta})$, then the derivative may be written as:
\begin{eqnarray}
    \frac{\partial K_{ii}}{\partial w} &=& \delta_{ij}\\
    \frac{\partial K_{ij}}{\partial \alpha_k} &=&
     \left\{
    \begin{array}{cc}
       1 & j = i \\
        \exp{(-\beta_k |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.\\
    \frac{\partial K_{ij}}{\partial \beta_k} &=&
     \left\{
    \begin{array}{cc}
       0  & j = i \\
        -\alpha_k|t_i-t_j|\exp{(-\beta_k |t_i-t_j|)}  & j \ne i
    \end{array}
    \right.
\end{eqnarray}
The first term in the derivative of the likelihood can be computed in $O(N)$
evaluations, while the second term can be computed in three steps:
\begin{itemize}
\item First, solve ${\bf z_1} = {\bf K}^{-1}{\bf y}$ with the GRP solver.
\item Next, create an e
\end{itemize}
Writing this term out:

\section{Summary}

\vspace{1.5em}
All of the code used in this project is available from
\url{https://github.com/dfm/GenRP} under the MIT open-source software
license.
This code (plus some dependencies) can be run to re-generate all of the
figures and results in this paper; this version of the paper was generated
with git commit \texttt{\githash} (\gitdate).

\section{Solving for log likelihood with extended matrix}

Here we summarize \citet{Ambikasaran2015}, and then point to appendix
for how to do this with oscillating kernels.

\section{Training}

May use derivative of likelihood + BFGS-B to optimize kernel parameters.

Q: How do we determine how many kernels to use?

\section{Prediction/interpolation}

Precition/interpolation involves constructing a matrix of covariances
between the (noisy) training set and the times at which we wish to
predict or interpolate data points.   If our training times are given by
${\bf t}$, while the predicted times are given by ${\bf t}_*$, then
mean values at the predicted times are given by (RW 2.23):
\begin{eqnarray}
\bar f_{*,k} &=& \sum_j K(t_{*,k},t_j) b_j,\\
&=& \sum_j \sum_p \alpha_p e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
&=& \sum_p \alpha_p \sum_j e^{-\beta_p \vert t_{*,k}-t_j\vert} b_j,\\
y_k &=& \sum_j K(t_j,t_k) b_j,
\end{eqnarray}
where the latter equation may be solved for ${\bf b}$ using the extended
matrix formalism.  Let the length of ${\bf t}_*$ be $M$; then, the
matrix $K({\bf t}_*,{\bf t})$ has a size $M \times N$, and so a total
of $MN$ multiplications is required to obtain the predicted values.
If $M$ is large, this can become quickly prohibitive.  It turns out
that the structure of the sum of exponential kernels may be exploited
to obtain the predicted mean in of order $p(M+N)+p^2N$ multiplications,
i.e.\ a linear scaling with the total number of data points, which can 
be much fewer if $M$ is large.

The procedure works as follows:
\begin{itemize}
\item Sort ${\bf t}$ and ${\bf t}_*$ in time order.
\item Starting with the smallest value of ${\bf t}_*$, $t_{*,1}$,
compute:
\begin{eqnarray} \label{prediction}
\bar f_{*,k} &=& \sum_p \alpha_p \left[\epsilon^+_{p,k} + \epsilon^-_{p,k}\right]\\
\epsilon^+_{p,k} &=& \sum_{j \forall t_{*,k} > t_j} e^{-\beta_p (t_{*,k}-t_j)} b_j\\
\epsilon^-_{p,k} &=& \sum_{j \forall t_{*,k} \le t_j} e^{-\beta_p (t_j-t_{*,k})} b_j,
\end{eqnarray}
where we have separated out the training times before and after the predicted time $t_{*,k}$.

\item Next, we update the coefficients recursively:
\begin{eqnarray}
\epsilon^+_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^+_{p,k-1} + \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_{*,k}-t_j)} b_j\right]\\
\epsilon^-_{p,k} &=& e^{-\beta_p (t_{*,k}-t_{*,k-1})} \left[\epsilon^-_{p,k-1} - \sum_{j \forall t_{*,k} > t_j \ge t_{*,k-1}} e^{-\beta_p (t_j-t_{*,k})} b_j\right].
\end{eqnarray}
With these updates, we can then use equation \ref{prediction} to compute the expected
mean at $t_{*,k}$ with an additional $p$ operations.  Since there are $N$ values of
$t_k$, the number of operations needed to update $\epsilon$ is of order $p(N+M)$, and
so this procedure avoids a heavy computational burden.

The variance may be handled in a similar manner, but with $p^2$ coefficients
to recursively update. For example, if $t_{*,k} > t_j \forall j,k$ then:
\begin{eqnarray}
cov(f_{*,k}) &=&  K(t_{*,k},t_{*,k}) - \sum_p \sum_q \alpha_p \alpha_q e^{-\beta_p(t_{*,k}-{\bf t}^T)} K({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
&=& K(0) - \sum_p \sum_q \alpha_p \alpha_q \zeta_{p,q,k},\\
\zeta_{p,q,k} &=&   e^{-\beta_p(t_{*,k}-{\bf t}^T)} K({\bf t},{\bf t})^{-1} e^{-\beta_q (t_{*,k}-{\bf t})},\\
&=&  e^{-(\beta_p+\beta_q)(t_{*,k}-t_{*,k-1})} \zeta_{p,q,k-1}.
\end{eqnarray}
The recursive updates to the $\zeta_{p,q,k}$ parameters can be made at each step.
\end{itemize}

Problem:  if we have the case that the datapoints are interspersed, and if the sign changes,
then we need to resolve $K^{-1} e^{-\beta_q \vert t_{*,k}-{\bf t}\vert}$.  However, maybe we can
just solve for the difference in fewer operations? {\bf TBD}

\acknowledgments
It is a pleasure to thank
...
for helpful contributions to the ideas and code presented here.

EA acknowledges support from NASA grants NNX13AF20G, NNX13AF62G, and
NASA Astrobiology Institute's Virtual Planetary Laboratory, supported
by NASA under cooperative agreement NNH05ZDA001C.

This research made use of the NASA \project{Astrophysics Data System} and the
NASA Exoplanet Archive.
The Exoplanet Archive is operated by the California Institute of Technology,
under contract with NASA under the Exoplanet Exploration Program.

This paper includes data collected by the \kepler\ mission. Funding for the
\kepler\ mission is provided by the NASA Science Mission directorate.
We are grateful to the entire \kepler\ team, past and present.

These data were obtained from the Mikulski Archive for Space Telescopes
(MAST).
STScI is operated by the Association of Universities for Research in
Astronomy, Inc., under NASA contract NAS5-26555.
Support for MAST is provided by the NASA Office of Space Science via grant
NNX13AC07G and by other grants and contracts.

\facility{Kepler}
\software{%
    % \project{ceres} \citep{Agarwal:2016},
    % \project{corner.py} \citep{Foreman-Mackey:2016},
    % \project{emcee} \citep{Foreman-Mackey:2013},
    % \project{george} \citep{Ambikasaran2016},
	% \project{matplotlib} \citep{Hunter:2007},
	% \project{numpy} \citep{Van-Der-Walt:2011},
	% \project{scipy} \citep{Jones:2001}.
}

\appendix

Here we adapt the generalized Rybicki-Press (GRP) algorithm to handle periodic kernels.  We
follow closely the notation and equations given in \citet{Ambikasaran2015}.  We start
with the observation that a Lorentzian correlation function may be decomposed into the sum of two
exponential functions:
\begin{eqnarray}
K(\tau) &=& \alpha e^{-\beta_R \tau} \cos{(\omega \tau)}\\
 &=& \frac{1}{2}\alpha \left[e^{-\beta \tau}+e^{-\beta^* \tau}\right],
\end{eqnarray}
where $\beta = \beta_R + i\beta_I$ is the 
complex kernel parameter (i.e.\ $\beta_R = Re(\beta)$, $\beta_I = Im(\beta)$), $\omega = -\beta_I = 2\pi/P$
is the frequency of oscillation of the kernel with period $P$, and $x^*$ is the complex conjugate of $x$.

Now, the Lorentzian kernal may be included in the GRP algorithm 
using the two exponential kernels with $\beta_1 = \beta$ and $\beta_2 = \beta^*$.  This 
requires the algorithm to use complex arithmetic, which has several disadvantages:
1).\ complex arithmetic is more expensive by a factor of 2-3; 2).\ the complex conjugate equation
is redundant as it turns out that $l_2 = l_1^*$, $r_2 = r_1^*$, and $x_i = x_i^*$ (i.e.\ $x_i$ is
real), meaning that the same equations are being solved twice for the real and complex components; 
and 3).\ if the derivative of the likelihood is computed with automatic
differentiation (AD), most AD algorithms require real variables.

Given these drawbacks, we instead modify the GRP algorithm to use a combination of two real
components rather than two complex components;  this addresses all three of these drawbacks
simultaneously.  The one cost is that the banded extended matrix has a bandwidth which is
larger by one, both above and below the diagonal.

Rather than using $\beta$ and its complex conjugate as components of the kernel, we instead
find the real and imaginary components of the complex equation for $\beta$ by taking the
sum and difference of the $\beta$ and $\beta^*$ equations (divided by 2 and $2i$, respectively), 
and then discard the $\beta^*$ equation, which is redundant.  This keeps the same number of
additional equations for each $\beta_j$ with non-zero imaginary component (four additional
equations in the extended matrix), but converts the complex equations to real, thus avoiding
complex arithmetic, while the kernel components with real $\beta_j$ still only have two 
additional components in the extended matrix.  The extended matrix adds an extra band above
and below the diagonal when the $\beta$ and $\beta^*$ equations are combined since this
mixes the components of the original equations which are shifted by one column from one
another.  We will enumerate the number of real $\beta$'s as $p_0$, while the total number 
of $\beta$'s is still $p$.  This gives a total number of equations to be solved with the 
extended matrix of $N_{ex}= (4(p-p_0)+2p_0+1)\times(N-1)+1$, where $N$ is the number of data 
points.  

For the complex values of $\beta$, the equations are modified as follows.  We introduce complex
auxiliary vectors $l_k = l_{k}^R - i l_{k}^I$, with $k \in \{1,2,...,N\}$ and $l_1 = 0$; $r_k =
r_k^R - i r_k^I$, with $k \in \{2,...,N\}$ and $r_{N+1} = 0$.  Note that we have defined the
imaginary component as being negative for aesthetic reasons:  this yields a symmetric extended
matrix (unfortunately the extended matrix has negative eigenvalues, making it non positive-definite,
so that Cholesky decomposition cannot be applied; nevertheless we prefer to use a symmetric 
set of equations, and since the values of these variables are discarded in the end, their sign
is irrelevant).  The length of the vectors $l_k$ and $r_k$
is the number of kernel components, $p$;  we denote the individual elements of these vectors
as $l_{k,j}$ and $r_{k,j}$, $j \in \{1,...,p\}$.  Note that the first $p_0$ elements we take
to be the real values of $\beta$, and for these we do not include the imaginary component
equations, which are unnecessary and only add extra computation time.  We allow each vector $\gamma_k$ to have real and imaginary
components as well:  $\gamma_k = \left[\exp{(-\beta_1 t_{k,k+1})} ... \exp{(-\beta_p t_{k,k+1})}\right]^T
 = \gamma_k^R + i \gamma_k^I.$  These elements are given by $\gamma_{k,j}^R = e^{-\beta_j t_{k,k+1}}
\cos{(\beta_j^I t_{k,k+1})}$ and $\gamma_{k,j}^I = -e^{-\beta_j t_{k,k+1}}\sin{(\beta_j^I t_{k,k+1})}$.
Equation (61) becomes:
\begin{equation} \label{Amb61}
% See 9/2/2016 notes
\sum_{j=1}^p \frac{\alpha_i}{2} l_{k,j}^R + \frac{d}{2} x_k + \sum_{j=1}^p \left[
\gamma_{k,j}^R r_{k+1,j}^R + \gamma_{k,j}^I r_{k+1,j}^I\right] = \frac{b_k}{2},
\end{equation}
for $j \in \{1,...,p\}$ and $k \in \{1,...,N\}$.  There is no imaginary component to equation (61)
in \citet{Ambikasaran2015}.

Equation (60) in \citet{Ambikasaran2015} has real and imaginary components:
\begin{eqnarray} \label{Amb60}
\gamma_{k,j}^R l_{k,j}^R +\gamma_{k,j}^I l_{k,j}^I + \gamma_{k,j}^R x_k - l_{k+1,j}^R &=& 0,\\
\gamma_{k,j}^I l_{k,j}^R -\gamma_{k,j}^R l_{k,j}^I + \gamma_{k,j}^I x_k + l_{k+1,j}^I &=& 0,
\end{eqnarray}
with $j \in \{1,...,p\}$ and $k \in \{1,...,N-1\}$
\citep[note that we have shifted the indices of this equation by one from the indexing using in][]{Ambikasaran2015}.

Finally, equation (59) in \citet{Ambikasaran2015} has real and imaginary components:
\begin{align} \label{Amb59}
-&r_{k,j}^R + \frac{\alpha_j}{2} x_k &+ \gamma_{k,j}^R r_{k+1,j}^R + \gamma_{k,j}^I r_{k+1,j}^I &&=0,\\
 &r_{k,j}^I &+ \gamma_{k,j}^I r_{k+1,j}^R - \gamma_{k,j}^R r_{k+1,j}^I &&=0,
\end{align}
with $j \in \{1,...,p\}$ and $k \in \{2,...,N\}$.  Note that for the $p_0$ real components
we can drop the variables $r_{k,j}^I$ and $l_{k,j}^I$ as these are zero, and we can drop the
imaginary components of equations (59) and (60) in \citet{Ambikasaran2015} as well for these, which reduces the total number
of equations to solve.

With these additional equations, we can solve for the likelihood of sums of full Lorentzian kernels 
in $O(N)$ operations, but with slightly more computational expense due to having 4 extra variables 
for each Lorentzian with non-zero $\omega$, and due to having a slightly larger bandwidth due to the 
cross terms between the real and imaginary variables.  The off-diagonal components are $p_0+2(p-p_0)+2$ above 
and below the diagonal, for a total bandwidth of $2p_0+4(p-p_0)+5$. In the case in which $p_0=p$, so 
that the imaginary components of each $\beta$ are zero, the off-diagonal components revert to $p+1$ 
above and below the diagonal, giving a bandwidth of $2p+3$ (the same as the GRP algorithm).

As with the GRP method, we can build an extended matrix which is real and symmetric, and which may be 
solved with a banded solver, as in \citet{NumericalRecipes}.  The determinant of this extended matrix 
may be computed from the LU decomposition, and it is related to the determinant of the kernel by a 
factor of $2^N$ (this is due to dividing each equation by $2$ to yield a symmetric matrix).
We arrange the auxiliary variables with real values of 
$\beta$ first, followed by the complex variables, yielding the vector
$x_{ex} = [x_1, \{r_{2,j}^R,l_{2,j}^R;j=1,...,p_0\}, \{r_{2,j}^R,r_{2,j}^I,l_{2,j}^R,l_{2,j}^I;j=p_0+1,...,p\},
x_2, ..., x_N]$.  The right hand side is given by: $y_{ex} = [b_1/2, \{0; j=1,...,p_0+2(p-p_0)\}, b_2/2, ..., b_N/2]$.
The extended matrix, $A_{ex}$, is constructed by inserting the coefficients of equations \ref{Amb61},
\ref{Amb60}, and \ref{Amb59} into a matrix of dimensions $(2p_0+4(p-p_0)+5) \times N_{ex}$.
The solution is found by solving $A_{ex} x_{ex} = y_{ex}$ with LU decomposition and back-substitution
with a banded algorithm.

The matrix has a similar structure as the real case constructed by \citep{Ambikasaran2015}, but the
imaginary and real components mix (this is due to the representation of imaginary numbers as
$2\times 2$ matrices).  Figure \ref{matrix_structure} shows an example of the structure of the extended
matrix for two Lorentzians with non-zero $\omega$ values.  This is analagous to the real case with
$p=2$ which also has four additional equations for each row of the original kernel.  This matrix shows
that there is an extra band above and below the diagonal \citep[compare with Figure 2 of][]{Ambikasaran2015}.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.175]{./twoterm.eps}
\hspace{0.2in}
\includegraphics[scale=1.0]{./colorcode.eps}
\end{center}
\caption{Pictorial description of the extended sparse matrix where $N=10$, $p_0=0$, and $p=2$, following
the example shown in \citet{Ambikasaran2015}.}
\label{matrix_structure}
\end{figure}

{\bf Should I give some examples of this?  Some pseudocode?}

\bibliography{genrp}

\end{document}
